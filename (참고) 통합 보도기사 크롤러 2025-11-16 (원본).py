# Combined_News_Scraper_GUI_v5.0.1.py
# 6ëŒ€ ì§€ë°©ì§€ + 16ê°œ ê¸°íƒ€ ì§€ë°©ì§€ + Google ë‰´ìŠ¤ í†µí•© ìŠ¤í¬ë˜í¼
#
# [v5.0.1 ë³€ê²½ ì‚¬í•­]
# 1. (ìš”ì²­) [Step 1]ì— 'ì˜¤ëŠ˜ ë‚ ì§œ' ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ë²„íŠ¼ ì¶”ê°€ (ë‚ ì§œ ë™ì  í‘œì‹œ)
# 2. (ìš”ì²­) [Step 1]ì— 'ì—‘ì…€íŒŒì¼ë¡œ ë‹¤ìš´' ë²„íŠ¼ ì¶”ê°€
# 3. (ìš”ì²­) [Step 1] GUIëŠ” (ì œëª©, ë‚ ì§œ, í‚¤ì›Œë“œ)ë¡œ v5.0 í˜•ì‹ ìœ ì§€
# 4. (ìš”ì²­) [Step 1] 'ì—‘ì…€ ë‹¤ìš´' ì„œì‹ì€ (ì—°ë²ˆ, ì§€ì—­, ì‹œê°„, ê¸°ê´€ ë“±) v1.4 í˜•ì‹ ì ìš©
# 5. ì´ë¥¼ ìœ„í•´ [Step 1]ì˜ 'scrape_press_releases' í•¨ìˆ˜ë¥¼ v1.4 ê¸°ì¤€ìœ¼ë¡œ êµì²´ (íŒŒì‹± ì •ë³´ í™•ì¥)
# 6. 'extract_keyword' (GUIìš©)ì™€ 'extract_institution' (ì—‘ì…€ìš©) í•¨ìˆ˜ ë™ì‹œ ì‚¬ìš©

# =============================================================================
# 1. ëª¨ë“  Import ë¬¸ í†µí•©
# =============================================================================
import time
import re
import pandas as pd
import traceback
from datetime import datetime, timedelta
from urllib.parse import quote, urlencode, urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
import os
import sys
import subprocess

# --- [v5.0] Google ë‰´ìŠ¤ìš© feedparser ì¶”ê°€ ---
import feedparser

# --- [v5.0] ë³´ë„ìë£Œ ìŠ¤í¬ë ˆì´í¼ìš© ëª¨ë“ˆ ---
import requests
import ssl
from requests.adapters import HTTPAdapter

# --- [v5.0] Selenium ë° WebDriverManager ëª¨ë“ˆ ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# --- [v5.0] ë³‘ë ¬ ì²˜ë¦¬ ë° GUI ëª¨ë“ˆ ---
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import tkinter as tk
# âœ… [v5.0.1 ìˆ˜ì •] filedialog (ì—‘ì…€ ì €ì¥), Toplevel (íŒì—…) ì¶”ê°€
from tkinter import ttk, messagebox, Toplevel, filedialog
import webbrowser
import threading

# âœ… [v5.0.1 ì‹ ê·œ] ì—‘ì…€ ì €ì¥ìš© ëª¨ë“ˆ
import xlsxwriter
# âœ… Word ë¬¸ì„œ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE

# =============================================================================
# 2. ê³µí†µ í—¬í¼ í•¨ìˆ˜ (ë³´ë„ìë£Œ ìŠ¤í¬ë ˆì´í¼)
# =============================================================================

class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
        try:
            context.set_ciphers('DEFAULT@SECLEVEL=1')
        except ssl.SSLError:
            print("ê²½ê³ : set_ciphers('DEFAULT@SECLEVEL=1') ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        kwargs['ssl_context'] = context
        return super(LegacyTLSAdapter, self).init_poolmanager(*args, **kwargs)

# âœ… [v5.0.1] v5.0ì˜ 'extract_keyword' (GUIìš©) í•¨ìˆ˜ ìœ ì§€
def extract_keyword(title):
    try:
        # âœ… [v4.0] íŠ¹ì • í•™êµ ì´ë¦„ì´ ì œëª©ì— í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ ì´ë¦„ì„ ìµœìš°ì„  ë°˜í™˜
        specific_schools = ["í•œì‚¬ë‘í•™êµ", "ì„±ê´‘í•™êµ", "ê´‘ì£¼ìƒˆë¡¬í•™êµ", "ë™í˜„í•™êµ", "ì¸ë•í•™êµ"]
        for school in specific_schools:
            if school in title:
                return school # (1ìˆœìœ„) íŠ¹ì • í•™êµ ì´ë¦„ ë°˜í™˜

        # (ì´í•˜ ê¸°ì¡´ ë¡œì§ - 2ìˆœìœ„)
        if "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­" in title:
            return "ê´‘ì£¼í•˜ë‚¨êµìœ¡"
        
        # (ì´í•˜ ê¸°ì¡´ ë¡œì§ - 3ìˆœìœ„)
        match = re.search(r'\b([\w]+(ì´ˆ|ì¤‘|ê³ |ìœ |ë³‘ì„¤ìœ |ì´ˆë³‘ì„¤ìœ |ë³‘ì„¤ìœ ì¹˜ì›|ì´ˆë“±í•™êµ|ì¤‘í•™êµ|ê³ ë“±í•™êµ|ìœ ì¹˜ì›))\b', title)
        
        if match:
            keyword = match.group(1)
            return keyword
        
        # (ì´í•˜ ê¸°ì¡´ ë¡œì§ - 4ìˆœìœ„)
        first_part = title.split(',')[0].strip()
        first_word = first_part.split(' ')[0].strip()
        
        if len(first_word) > 10:
            return first_word[:10]
        
        return first_word

    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e} (ì œëª©: {title})")
        return title[:5]

# âœ… [v5.0.1 ì‹ ê·œ] ì—‘ì…€ ì €ì¥ìš© 'ë“±ë¡ê¸°ê´€' ì¶”ì¶œ í•¨ìˆ˜ (from ë·°ì–´ v1.4)
def extract_institution(title):
    """ (v1.4 ë¡œì§) """
    try:
        specific_schools = ["í•œì‚¬ë‘í•™êµ", "ì„±ê´‘í•™êµ", "ê´‘ì£¼ìƒˆë¡¬í•™êµ", "ë™í˜„í•™êµ", "ì¸ë•í•™êµ"]
        for school in specific_schools:
            if school in title:
                return school 
        if "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­" in title:
            return "ê´‘ì£¼í•˜ë‚¨êµìœ¡"
        match = re.search(r'\b([\w]+(ì´ˆ|ì¤‘|ê³ |ìœ |ë³‘ì„¤ìœ |ì´ˆë³‘ì„¤ìœ |ë³‘ì„¤ìœ ì¹˜ì›|ì´ˆë“±í•™êµ|ì¤‘í•™êµ|ê³ ë“±í•™êµ|ìœ ì¹˜ì›))\b', title)
        if match:
            return match.group(1)
        first_part = title.split(',')[0].strip()
        first_word = first_part.split(' ')[0].strip()
        return first_word[:10] if len(first_word) > 10 else first_word
    except Exception as e:
        print(f"ë“±ë¡ê¸°ê´€ ì¶”ì¶œ ì˜¤ë¥˜: {e} (ì œëª©: {title})")
        return title[:5]


# âœ… [v5.0.1 ìˆ˜ì •] v1.4ì˜ ì—‘ì…€ ì €ì¥ìš© ì •ë³´ê¹Œì§€ íŒŒì‹±í•˜ë„ë¡ í•¨ìˆ˜ êµì²´
def scrape_press_releases(base_url):
    """
    [v5.0.1 ìˆ˜ì •]
    ì§€ì •ëœ base_urlì˜ 1í˜ì´ì§€ì—ì„œ ë³´ë„ìë£Œë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    - GUIìš© 'keyword_gui' (from extract_keyword)
    - ì—‘ì…€ìš© 'institution', 'region' ë“± (from extract_institution)
    - [ìš”ì²­ 1] ë“±ë¡ì‹œê°„ íŒŒì‹± ì œê±°.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    session = requests.Session()
    session.mount("https://", LegacyTLSAdapter())
    session.mount("http://", LegacyTLSAdapter())
    results_list = []
    total_articles_found = 0

    try:
        # 1í˜ì´ì§€ë§Œ ìˆ˜ì§‘
        page_url = f"{base_url}&pageIndex=1"
        print(f"[ë³´ë„ìë£Œ] 1í˜ì´ì§€ ìŠ¤í¬ë˜í•‘... (URL: {page_url})")

        response = session.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('a.selectNttInfo')

        if not articles:
            print(f"[ë³´ë„ìë£Œ] 1í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        for article in articles:
            original_title = article.get('title', 'ì œëª© ì—†ìŒ').strip()
            
            # --- ë‚ ì§œ/ì‹œê°„ íŒŒì‹± (v1.4 ê¸°ì¤€) ---
            date_tag = article.find('span', class_='date')
            date_str_raw = date_tag.get_text(strip=True) if date_tag else 'ë‚ ì§œ ì—†ìŒ'
            date = "ë‚ ì§œì—†ìŒ"
            time = "ì‹œê°„ì—†ìŒ" # (v1.4) ìŠ¤ë ˆë“œ ì‹¤í–‰ ì‹œê°„ìœ¼ë¡œ ë®ì–´ì“¸ ì˜ˆì •
            
            date_match = re.search(r"(\d{4}\.\d{2}\.\d{2})", date_str_raw)
            if date_match:
                date = date_match.group(0)
            else:
                print(f"âš ï¸ [ë³´ë„ìë£Œ] ë‚ ì§œ í˜•ì‹ ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€: {date_str_raw}")
                continue

            # --- ë§í¬ íŒŒì‹± (v1.4 ê¸°ì¤€) ---
            nttSn = article.get('data-param') 
            link = "ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨" 
            if nttSn and nttSn.isdigit():
                link = f"https://www.goegh.kr/goegh/na/ntt/selectNttInfo.do?mi=8686&bbsId=5041&nttSn={nttSn}"
            else:
                print(f"âš ï¸ [ë³´ë„ìë£Œ] ë§í¬(nttSn) ì¶”ì¶œ ì‹¤íŒ¨. (data-param ê°’: {nttSn})")

            # --- ì œëª©/ì§€ì—­/ê¸°ê´€ íŒŒì‹± (v1.4 ê¸°ì¤€) ---
            clean_title = original_title 
            title_match = re.search(r'^ë³´ë„ìë£Œ\((.*)\)$', original_title)
            if title_match:
                clean_title = title_match.group(1).strip() 
            
            region = "êµìœ¡ì§€ì›ì²­"
            if clean_title.startswith("ê´‘ì£¼ "): region = "ê´‘ì£¼"
            elif clean_title.startswith("í•˜ë‚¨ "): region = "í•˜ë‚¨"
            elif "ê´‘ì£¼í•˜ë‚¨" in clean_title: region = "êµìœ¡ì§€ì›ì²­"

            # âœ… ì—‘ì…€ ì €ì¥ìš© (v1.4)
            institution = extract_institution(clean_title)
            # âœ… GUI í‘œì‹œìš© (v5.0)
            keyword_gui = extract_keyword(clean_title) 
            
            results_list.append({
                "priority": "",
                "region": region,
                "date": date,
                "time": time,
                "title": clean_title,
                "institution": institution, # ì—‘ì…€ìš©
                "keyword_gui": keyword_gui, # GUIìš©
                "link": link, 
                "notes": ""
            })
            total_articles_found += 1
        
        print(f"ì´ {total_articles_found}ê°œì˜ ê²Œì‹œê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (1 í˜ì´ì§€)")
        return results_list

    except requests.exceptions.RequestException as e:
        print(f"HTTP ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        traceback.print_exc()
    return []

# =============================================================================
# 3. [v5.0] Selenium ë“œë¼ì´ë²„ ì„¤ì • (6ëŒ€, 16ëŒ€ ê³µí†µ ì‚¬ìš©)
# (ì´í•˜ v5.0 ì›ë³¸ê³¼ ë™ì¼ ... setup_driver, setup_driver_compatible)
# =============================================================================

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

@lru_cache(maxsize=1)
def get_driver_path():
    print("WebDriverManager: í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì¹˜ ë˜ëŠ” ì—…ë°ì´íŠ¸ í™•ì¸ ì¤‘...")
    try:
        path = ChromeDriverManager().install()
        print(f"WebDriverManager: ë“œë¼ì´ë²„ ê²½ë¡œ í™•ì¸: {path}")
        return path
    except Exception as e:
        print(f"WebDriverManager: ë“œë¼ì´ë²„ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
        print("PATHì— chromedriverê°€ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return "chromedriver"

def setup_driver(headless=True):
    """
    ìµœì í™”ëœ ì˜µì…˜ìœ¼ë¡œ í¬ë¡¬ ì›¹ ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (ëŒ€ë¶€ë¶„ì˜ ì‚¬ì´íŠ¸ìš©)
    (from 'ê¸°íƒ€ì§€ë°©ì§€ í¬ë¡¤ëŸ¬' - prefs í¬í•¨ ë²„ì „)
    """
    options = Options()
    if headless:
        options.add_argument("--headless=new")
    
    options.add_argument(f"--user-agent={USER_AGENT}")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--ignore-ssl-errors")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-component-extensions-with-background-pages")
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-sync")

    # [v5.0] 'ê¸°íƒ€ì§€ë°©ì§€'ì˜ ë¦¬ì†ŒìŠ¤ ìµœì í™”(prefs) ì˜µì…˜ ì‚¬ìš©
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
        "profile.managed_default_content_settings.cookies": 2,
        "profile.managed_default_content_settings.javascript": 1,
        "profile.managed_default_content_settings.plugins": 2,
        "profile.managed_default_content_settings.popups": 2,
        "profile.managed_default_content_settings.geolocation": 2,
        "profile.managed_default_content_settings.notifications": 2,
        "profile.managed_default_content_settings.media_stream": 2,
    }
    options.add_experimental_option("prefs", prefs)
    
    try:
        service = Service(get_driver_path()) 
        return webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"ë“œë¼ì´ë²„ ì‹œì‘ ì˜¤ë¥˜: {e}")
        print("ë“œë¼ì´ë²„ ë²„ì „ì´ í¬ë¡¬ ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

def setup_driver_compatible(headless=True):
    """
    í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨(prefs) ì˜µì…˜ì„ ì œê±°í•œ ë“œë¼ì´ë²„ì…ë‹ˆë‹¤.
    (from 'ê¸°íƒ€ì§€ë°©ì§€ í¬ë¡¤ëŸ¬')
    """
    options = Options()
    if headless:
        options.add_argument("--headless=new")
    
    options.add_argument(f"--user-agent={USER_AGENT}")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--ignore-ssl-errors")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-component-extensions-with-background-pages")
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-sync")
    
    try:
        service = Service(get_driver_path()) 
        return webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"ë“œë¼ì´ë²„ ì‹œì‘ ì˜¤ë¥˜: {e}")
        print("ë“œë¼ì´ë²„ ë²„ì „ì´ í¬ë¡¬ ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

# =============================================================================
# 4. [v5.0] Fetcher í•¨ìˆ˜ ê·¸ë£¹ 1 (ì£¼ìš” 6ëŒ€ ì§€ë°©ì§€)
# (ì´í•˜ v5.0 ì›ë³¸ê³¼ ë™ì¼ ... fetch_kiho_multi ~ fetch_joongbu_multi)
# =============================================================================

# --- ê¸°í˜¸ì¼ë³´ ìŠ¤í¬ë ˆì´í¼ (fetch_kiho_multi) ---
def fetch_kiho_multi(keywords, date_limit, days_limit):
    
    ARTICLE_LIST_SELECTOR = "li.altlist-text-item"
    TITLE_SELECTOR = "h2.altlist-subject a"
    DATE_SELECTOR = "div.altlist-info div.altlist-info-item"
    IS_DATE_IN_LIST = True 
    DATE_INDEX = 2 
    
    source_name = "ê¸°í˜¸ì¼ë³´"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []
    
    try:
        for keyword in keywords:
            url = f"https://www.kihoilbo.co.kr/news/articleList.html?sc_word={quote(keyword)}&sc_order_by=E"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")
            
            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until( 
                    EC.presence_of_element_located((By.CSS_SELECTOR, ARTICLE_LIST_SELECTOR))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select(ARTICLE_LIST_SELECTOR)
                found_count = 0
                
                for item in articles:
                    title = "" 
                    try:
                        title_tag = item.select_one(TITLE_SELECTOR)
                        if not title_tag: continue 
                        title = title_tag.get_text(strip=True)
                        if not title: continue 
                        link = title_tag["href"]
                        if not link.startswith("http"):
                            link = urljoin("https://www.kihoilbo.co.kr", link) 
                        
                        date_str_raw = "" 
                        if IS_DATE_IN_LIST:
                            info_items = item.select(DATE_SELECTOR)
                            if len(info_items) <= DATE_INDEX:
                                continue
                            date_str_raw = info_items[DATE_INDEX].get_text(strip=True) 
                        else:
                            date_tag = item.select_one(DATE_SELECTOR)
                            if not date_tag: continue
                            date_str_raw = date_tag.get_text(strip=True)
                        
                        if not date_str_raw: continue 

                        date_str = date_str_raw.split(" ")[0] 
                        
                        if re.match(r"^\d{4}\.\d{2}\.\d{2}$", date_str):
                            date_obj = datetime.strptime(date_str, "%Y.%m.%d")
                        elif re.match(r"^\d{4}-\d{2}-\d{2}$", date_str):
                            date_obj = datetime.strptime(date_str, "%Y-%m-%d")
                        else:
                            continue
                        
                        if date_obj >= date_limit:
                            results.append({"ë³´ë„ì¼": date_obj.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ 1ê°œ íŒŒì‹± ì˜¤ë¥˜: {e} (ì œëª©: {title})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })

            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })

    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ë˜ëŠ” ë“œë¼ì´ë²„ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results


# --- ì¸ì²œì¼ë³´ ìŠ¤í¬ë ˆì´í¼ (fetch_incheonilbo_multi) ---
def fetch_incheonilbo_multi(keywords, date_limit, days_limit):
    source_name = "ì¸ì²œì¼ë³´"
    base_url = "https://www.incheonilbo.com"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []
    
    try:
        for keyword in keywords:
            url = f"https://www.incheonilbo.com/news/articleList.html?sc_word={quote(keyword)}&sc_order_by=E"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")
            
            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.type1 > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("ul.type1 > li")
                found_count = 0

                for article in articles:
                    title = ""
                    try:
                        title_tag = article.select_one("h2.titles")
                        link_tag = article.select_one("a")
                        if not title_tag or not link_tag: continue
                        title = title_tag.text.strip()
                        if not title: continue 
                        link = urljoin(base_url, link_tag["href"])
                        date_str_tag = article.select_one("em.info.dated")
                        pub_date = None

                        if date_str_tag:
                            date_text = date_str_tag.text.strip()
                            if not date_text: continue 
                            try:
                                pub_date = datetime.strptime(date_text.split(" ")[0], "%Y.%m.%d")
                            except ValueError:
                                if "ë¶„ ì „" in date_text:
                                    minutes_ago = int(re.search(r'(\d+)', date_text).group(1))
                                    pub_date = datetime.now() - timedelta(minutes=minutes_ago)
                                elif "ì‹œê°„ ì „" in date_text:
                                    hours_ago = int(re.search(r'(\d+)', date_text).group(1))
                                    pub_date = datetime.now() - timedelta(hours=hours_ago)
                                else:
                                    continue
                        else:
                            continue

                        if pub_date and pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e} (ì œëª©: {title})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })

            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })
            
    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# --- ê²½ê¸°ì¼ë³´ ìŠ¤í¬ë ˆì´í¼ (fetch_kyeonggi_multi) ---
def fetch_kyeonggi_multi(keywords, date_limit, days_limit):
    source_name = "ê²½ê¸°ì¼ë³´"
    base_url = "https://www.kyeonggi.com"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []
    
    try:
        for keyword in keywords:
            url = f"https://www.kyeonggi.com/search?searchText={quote(keyword)}"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")

            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.article_list div.media"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("div.article_list div.media")
                found_count = 0
                
                for article in articles: 
                    title = ""
                    date_str = ""
                    try:
                        title_tag = article.select_one("h3 a")
                        if not title_tag: continue
                        title = title_tag.get_text(strip=True)
                        if not title: continue 
                        link = urljoin(base_url, title_tag.get("href", "")) 
                        date_tag = article.select_one("span.byline") 
                        if not date_tag: continue
                        
                        date_text_raw = date_tag.get_text() 
                        if not date_text_raw: continue

                        date_match = re.search(r"(\d{4}\.\d{2}\.\d{2})", date_text_raw)
                        if date_match:
                            date_str = date_match.group(1).replace(".", "-")
                        else:
                            date_elements = date_tag.select("span")
                            if len(date_elements) >= 3:
                                date_str_raw = date_elements[2].get_text(strip=True)
                            elif len(date_elements) >= 2:
                                date_str_raw = date_elements[1].get_text(strip=True)
                            else:
                                continue
                            date_str = date_str_raw.split(" ")[0].replace(".", "-")
                        
                        if not re.match(r"^\d{4}-\d{2}-\d{2}$", date_str):
                            continue
                        
                        pub_date = datetime.strptime(date_str, "%Y-%m-%d")
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e} (ë‚ ì§œ: {date_str}, ì œëª©: {title})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# --- ê²½ì¸ì¼ë³´ ìŠ¤í¬ë ˆì´í¼ (fetch_kyeongin_multi) ---
def fetch_kyeongin_multi(keywords, date_limit, days_limit):
    source_name = "ê²½ì¸ì¼ë³´"
    base_url = "https://www.kyeongin.com"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []
    
    try:
        for keyword in keywords:
            url = f"https://www.kyeongin.com/search?query={quote(keyword)}"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")

            driver.get(url)
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.search-arl-001 ul li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("div.search-arl-001 ul li")
                found_count = 0
                
                for item in articles:
                    title = ""
                    try:
                        a_tag = item.select_one("h4.title a")
                        if not a_tag: continue
                        title = a_tag.get_text(strip=True)
                        if not title: continue 
                        
                        href = a_tag["href"]
                        if href.startswith("//"):
                            link = "https:" + href
                        else:
                            link = urljoin(base_url, href)
                        
                        date_tag = item.select_one("span.date") 
                        if not date_tag: continue
                        date_str = date_tag.get_text(strip=True)
                        if not date_str: continue

                        pub_date = datetime.strptime(date_str, "%Y-%m-%d")
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e} (ì œëª©: {title})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# --- ê²½ê¸°ì‹ ë¬¸ ìŠ¤í¬ë ˆì´í¼ (fetch_kgnews_multi) ---
def fetch_kgnews_multi(keywords, date_limit, days_limit):
    source_name = "ê²½ê¸°ì‹ ë¬¸"
    base_url = "https://www.kgnews.co.kr"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []
    
    try:
        for keyword in keywords:
            url = f"https://www.kgnews.co.kr/news/search_result.html?search_mode=multi&s_title=1&s_sub_title=1&s_body=1&search={quote(keyword)}"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")

            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.art_list_all > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("ul.art_list_all > li")
                found_count = 0
                
                for article in articles:
                    title = ""
                    try:
                        title_tag = article.select_one("h2.clamp.c2")
                        link_tag = article.select_one("a")
                        info_tag = article.select_one("ul.ffd.art_info")
                        if not title_tag or not link_tag or not info_tag: continue
                        title = title_tag.text.strip()
                        if not title: continue 
                        link = urljoin(base_url, link_tag["href"]) 
                        date_text_raw = info_tag.text 
                        if not date_text_raw: continue
                        date_match = re.search(r"\d{4}\.\d{2}\.\d{2}", date_text_raw)
                        if not date_match: continue

                        pub_date = datetime.strptime(date_match.group(), "%Y.%m.%d")
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e} (ì œëª©: {title})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# --- ì¤‘ë¶€ì¼ë³´ ìŠ¤í¬ë ˆì´í¼ (fetch_joongbu_multi) ---
def fetch_joongbu_multi(keywords, date_limit, days_limit):
    source_name = "ì¤‘ë¶€ì¼ë³´"
    base_url = "https://www.joongboo.com"
    
    driver = setup_driver(headless=True)
    if not driver: return []
    results = []

    try:
        for keyword in keywords:
            url = f"https://www.joongboo.com/news/articleList.html?sc_area=A&sc_word={quote(keyword)}&sc_order_by=E"
            print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")

            driver.get(url)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-content"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("div.list-content")
                found_count = 0
                
                for article in articles:
                    title = ""
                    try:
                        title_tag = article.select_one("h4.titles a")
                        if not title_tag: continue
                        title = title_tag.get_text(strip=True)
                        if not title: continue 
                        link = urljoin(base_url, title_tag["href"])
                        date_elements = article.select("span.byline em")
                        if len(date_elements) < 2: continue
                        date_str = date_elements[1].text.strip().split(" ")[0]
                        if not date_str: continue

                        pub_date = datetime.strptime(date_str, "%Y.%m.%d")
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": source_name, "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e:
                        print(f"[{source_name}] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e} (ì œëª©: {title})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException as wait_e: 
                print(f"[{source_name}] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ).")
                results.append({
                    "ë³´ë„ì¼": "ì—†ìŒ", "ë³´ë„ë§¤ì²´": source_name,
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": url, "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e:
        print(f"[{source_name}] í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# =============================================================================
# 5. [v5.0] Fetcher í•¨ìˆ˜ ê·¸ë£¹ 2 (ê¸°íƒ€ 16ê°œ ì§€ë°©ì§€)
# (ì´í•˜ v5.0 ì›ë³¸ê³¼ ë™ì¼ ... fetch_kghottimes_multi ~ fetch_sudokwon_multi)
# =============================================================================

# fetch_kghottimes.py (ìµœì í™”)
def fetch_kghottimes_multi(keywords, date_limit, days_limit): 
    results = []
    driver = setup_driver(headless=True)
    if not driver: return []
    try:
        base_url = "http://ghottimenews.com/news/search_result.html"
        
        for keyword in keywords:
            search_url = f"{base_url}?search_mode=multi&s_title=1&s_body=1&search={quote(keyword)}"
            print(f"ğŸŒ [ê²½ê¸°í•«íƒ€ì„ìŠ¤] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")
            
            driver.get(search_url)
            
            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.art_list_all li"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                found_count = 0
                for li in soup.select("ul.art_list_all li"):
                    try:
                        title_tag = li.select_one("h2")
                        a_tag = li.select_one("a[href*='article.html']")
                        date_tag = li.select_one("li.date")
                        
                        if not title_tag or not a_tag or not date_tag: continue
                        
                        title = title_tag.get_text(strip=True)
                        link = "http://ghottimenews.com/news/" + a_tag["href"].split("/")[-1]
                        
                        date_match = re.search(r"\d{4}\.\d{2}\.\d{2}", date_tag.get_text(strip=True))
                        if not date_match: continue
                        
                        pub_date = datetime.strptime(date_match.group(), "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ê²½ê¸°í•«íƒ€ì„ìŠ¤", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e_item:
                        print(f"âš ï¸ [ê²½ê¸°í•«íƒ€ì„] ê°œë³„ íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ê²½ê¸°í•«íƒ€ì„ìŠ¤",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                
            except TimeoutException:
                print(f"ğŸ” [ê²½ê¸°í•«íƒ€ì„ìŠ¤] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ê²½ê¸°í•«íƒ€ì„ìŠ¤",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })

    except Exception as e_page:
        print(f"âŒ [ê²½ê¸°í•«íƒ€ì„] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit() 
            except Exception as e_quit: print(f"âš ï¸ [ê²½ê¸°í•«íƒ€ì„ìŠ¤] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_hanamtimes.py (ìµœì í™”)
def fetch_hanamtimes_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "http://www.hanamtimes.com"
    driver = setup_driver_compatible(headless=True) # í˜¸í™˜ì„± ë“œë¼ì´ë²„ ì‚¬ìš©
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword.encode('euc-kr'))
            search_url = f"{base_url}/news/articleList.html?sc_sub_section_code=S2N11&view_type=sm&sc_word={encoded_keyword}"
            print(f"ğŸŒ [í•˜ë‚¨íƒ€ì„ì¦ˆ] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td.list-titles"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("td.list-titles")
                found_count = 0
                seen_links = set()
                
                for title_td in articles:
                    try:
                        a_tag = title_td.select_one("a")
                        if not a_tag: continue
                        
                        title = a_tag.get_text(strip=True)
                        href = a_tag.get("href")
                        link = urljoin(search_url, href)
                        
                        if not link or link in seen_links: continue
                        seen_links.add(link)
                        
                        parent_tbody = a_tag.find_parent("tbody")
                        if not parent_tbody: continue
                        
                        date_tag = parent_tbody.select_one("td.list-times")
                        if not date_tag: continue
                        
                        date_text = date_tag.get_text(strip=True)
                        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", date_text)
                        
                        if date_match:
                            pub_date = datetime.strptime(date_match.group(1), "%Y-%m-%d")
                            if pub_date >= date_limit:
                                results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "í•˜ë‚¨íƒ€ì„ì¦ˆ", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                                found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [í•˜ë‚¨íƒ€ì„ì¦ˆ] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "í•˜ë‚¨íƒ€ì„ì¦ˆ",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´ ë§í¬ í™•ì¸)",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [í•˜ë‚¨íƒ€ì„ì¦ˆ] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": "íƒ€ì„ì•„ì›ƒ",
                    "ë³´ë„ë§¤ì²´": "í•˜ë‚¨íƒ€ì„ì¦ˆ",
                    "ë³´ë„ì œëª©": f"ì›¹ì‚¬ì´íŠ¸ ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤íŒ¨ (í‚¤ì›Œë“œ: '{keyword}')",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [í•˜ë‚¨íƒ€ì„ì¦ˆ] ë“œë¼ì´ë²„ ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [í•˜ë‚¨íƒ€ì„ì¦ˆ] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_ctnews.py (ìµœì í™”)
def fetch_ctnews_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.ctnews.co.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword.encode("euc-kr"))
            search_url = f"{base_url}/search.html?submit=submit&search={encoded_keyword}&search_and=1&search_exec=n_b&search_section=all&news_order=1"
            print(f"ğŸŒ [ì‹œí‹°ë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td[style*='padding'] a"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                rows = soup.select("td[style*='padding']")
                found_count = 0
                
                for row in rows:
                    try:
                        title_tag = row.select_one("a[href*='sub_read.html']")
                        date_tag = row.find_next("td", class_="data")
                        
                        if not (title_tag and date_tag): continue
                        
                        title = title_tag.get_text(strip=True)
                        href = title_tag.get("href", "")
                        full_link = f"{base_url}/{href.lstrip('/')}"
                        raw_date_text = date_tag.get_text(strip=True)
                        
                        date_match = re.search(r"\d{4}[/-]\d{2}[/-]\d{2}", raw_date_text)
                        if not date_match: continue
                        
                        pub_date = datetime.strptime(date_match.group().replace("/", "-"), "%Y-%m-%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ì‹œí‹°ë‰´ìŠ¤", "ë³´ë„ì œëª©": title, "ë§í¬": full_link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                    except Exception as e_item:
                        print(f"âš ï¸ [ì‹œí‹°ë‰´ìŠ¤] íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ì‹œí‹°ë‰´ìŠ¤",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ì‹œí‹°ë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ì‹œí‹°ë‰´ìŠ¤",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ì‹œí‹°ë‰´ìŠ¤] ë¡œë”© ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ì‹œí‹°ë‰´ìŠ¤] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_goodtimes.py (ìµœì í™” + ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ìˆ˜ì •)
def fetch_goodtimes_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.goodtimes.or.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        current_year = datetime.today().year
        for keyword in keywords:
            search_url = f"{base_url}/news/articleList.html?sc_area=A&view_type=sm&sc_word={quote(keyword)}"
            print(f"ğŸŒ [êµ¿íƒ€ì„ì¦ˆ] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.altlist-webzine-item"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                rows = soup.select("li.altlist-webzine-item")
                found_count = 0
                
                for row in rows:
                    try:
                        a_tag = row.select_one("h2.altlist-subject a")
                        date_tag_all = row.select("div.altlist-info-item")
                        
                        if not a_tag or not date_tag_all: continue
                        
                        title = a_tag.get_text(strip=True)
                        full_link = a_tag.get("href")
                        
                        pub_date = None
                        
                        for tag in date_tag_all:
                            date_text = tag.get_text(strip=True)
                            match = re.search(r"(\d{4})[.-](\d{2})[.-](\d{2})", date_text)
                            if match:
                                try:
                                    pub_date = datetime.strptime(match.group(0).replace(".", "-"), "%Y-%m-%d")
                                    break 
                                except Exception:
                                    continue
                        
                        if not pub_date:
                            for tag in date_tag_all:
                                date_text = tag.get_text(strip=True)
                                match = re.search(r"(\d{2})-(\d{2})", date_text) # MM-DD
                                if match:
                                    try:
                                        month, day = match.group(1), match.group(2)
                                        if 1 <= int(month) <= 12 and 1 <= int(day) <= 31:
                                            full_date_str = f"{current_year}-{month}-{day}"
                                            pub_date = datetime.strptime(full_date_str, "%Y-%m-%d")
                                            break
                                    except Exception:
                                        continue 
                        
                        if not pub_date:
                            print(f"âš ï¸ [êµ¿íƒ€ì„ì¦ˆ] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ì œëª©: {title[:20]}...)")
                            continue
                            
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "êµ¿íƒ€ì„ì¦ˆ", "ë³´ë„ì œëª©": title, "ë§í¬": full_link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [êµ¿íƒ€ì„ì¦ˆ] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë‚´ë¶€): {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "êµ¿íƒ€ì„ì¦ˆ",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [êµ¿íƒ€ì„ì¦ˆ] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ, ìš”ì†Œ ì—†ìŒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "êµ¿íƒ€ì„ì¦ˆ",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"ğŸ” [êµ¿íƒ€ì„ì¦ˆ] í…Œì´ë¸” ê¸°ì‚¬ ë¡œë”© ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [êµ¿íƒ€ì„ì¦ˆ] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_hnrsm.py (ìµœì í™”)
def fetch_hnrsm_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.hnrsm.com"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword)
            search_url = f"{base_url}/news/search.php?q={encoded_keyword}"
            print(f"ğŸŒ [í•˜ë‚˜ë¡œì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.news_list_skin > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("ul.news_list_skin > li")
                found_count = 0
                seen_links = set()
                
                for art in articles:
                    try:
                        a_tag = art.select_one("a")
                        title_tag = art.select_one("h3.line_int")
                        date_tag = art.select_one("p.date")
                        
                        if not a_tag or not title_tag or not date_tag: continue
                        
                        title = title_tag.get_text(strip=True)
                        href = a_tag.get("href")
                        link = base_url + href if href and href.startswith("/") else href
                        
                        if not link or link in seen_links: continue
                        seen_links.add(link)
                        
                        date_text = date_tag.get_text(strip=True)
                        pub_date = None
                        
                        for fmt in ("%Y-%m-%d", "%Y.%m.%d", "%Y-%m-%d %H:%M", "%Y.%m.%d %H:%M"):
                            try:
                                pub_date = datetime.strptime(date_text, fmt)
                                break
                            except: continue
                            
                        if pub_date and pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "í•˜ë‚˜ë¡œì‹ ë¬¸", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [í•˜ë‚˜ë¡œì‹ ë¬¸] ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "í•˜ë‚˜ë¡œì‹ ë¬¸",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [í•˜ë‚˜ë¡œì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "í•˜ë‚˜ë¡œì‹ ë¬¸",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [í•˜ë‚˜ë¡œì‹ ë¬¸] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [í•˜ë‚˜ë¡œì‹ ë¬¸] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_ehanam.py (ìµœì í™”)
def fetch_ehanam_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.ehanam.net"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            search_url = f"{base_url}/news/articleList.html?sc_area=A&view_type=sm&sc_word={quote(keyword)}"
            print(f"ğŸŒ [í•˜ë‚¨ì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.type2 > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                found_count = 0
                
                for li in soup.select("ul.type2 > li"):
                    try:
                        title_tag = li.find("h4", class_="titles")
                        if not title_tag: continue
                        a_tag = title_tag.find("a")
                        if not a_tag: continue
                        
                        title = a_tag.get_text(strip=True)
                        link = a_tag["href"]
                        if not link.startswith("http"):
                            link = base_url + link
                            
                        date_tag_span = li.find("span", class_="byline")
                        if not date_tag_span: continue
                        
                        date_em = date_tag_span.select_one("em:last-child")
                        date_text_full = date_tag_span.get_text()
                        
                        date_match = re.search(r"(\d{4}\.\d{2}\.\d{2})", date_em.get_text(strip=True)) if date_em else re.search(r"(\d{4}\.\d{2}\.\d{2})", date_text_full)
                        
                        if not date_match: continue
                        
                        pub_date = datetime.strptime(date_match.group(1), "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "í•˜ë‚¨ì‹ ë¬¸", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [í•˜ë‚¨ì‹ ë¬¸] ê°œë³„ íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "í•˜ë‚¨ì‹ ë¬¸",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [í•˜ë‚¨ì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "í•˜ë‚¨ì‹ ë¬¸",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
           print(f"âŒ [í•˜ë‚¨ì‹ ë¬¸] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [í•˜ë‚¨ì‹ ë¬¸] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_gjilbo.py (ìµœì í™”)
def fetch_gjilbo_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "http://www.gjilbo.com"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword.encode('euc-kr'))
            search_url = f"{base_url}/news/articleList.html?sc_word={encoded_keyword}"
            print(f"ğŸŒ [ê´‘ì£¼ì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a[href*='articleView']"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                found_count = 0
                
                for row in soup.select("table tr"):
                    try:
                        a_tag = row.select_one("a[href*='articleView']")
                        if not a_tag: continue
                        
                        title = a_tag.get_text(strip=True)
                        link = base_url + "/news/" + a_tag["href"]
                        
                        tds = row.select("td")
                        if not tds: continue
                        
                        date_tag = row.select_one("td.View_Sm_Date")
                        if not date_tag:
                            date_tag = tds[-1]
                        
                        date_text = date_tag.get_text(strip=True).split(" ")[0]
                        
                        if not re.match(r"\d{4}-\d{2}-\d{2}", date_text): continue
                        
                        pub_date = datetime.strptime(date_text, "%Y-%m-%d")
                        
                        if pub_date >= date_limit and keyword in title:
                            results.append({
                                "ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"),
                                "ë³´ë„ë§¤ì²´": "ê´‘ì£¼ì‹ ë¬¸",
                                "ë³´ë„ì œëª©": title,
                                "ë§í¬": link,
                                "ê²€ìƒ‰ì–´": keyword
                            })
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [ê´‘ì£¼ì‹ ë¬¸] ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ê´‘ì£¼ì‹ ë¬¸",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ê´‘ì£¼ì‹ ë¬¸] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ê´‘ì£¼ì‹ ë¬¸",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"ğŸ” [ê´‘ì£¼ì‹ ë¬¸] ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ê´‘ì£¼ì‹ ë¬¸] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_jungbusisa.py (ìµœì í™”)
def fetch_jungbusisa_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.gninews.co.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword)
            search_url = f"{base_url}/news/search_result.html?search={encoded_keyword}"
            print(f"ğŸŒ [ì¤‘ë¶€ì‹œì‚¬] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.art_list_all > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("ul.art_list_all > li")
                found_count = 0
                
                for article in articles:
                    try:
                        title_tag = article.select_one("h2.cmp.c2")
                        link_tag = article.select_one("a[href*='article.html']")
                        date_tag = article.select_one("li.date")
                        
                        if not (title_tag and link_tag and date_tag): continue
                        
                        title = title_tag.get_text(strip=True)
                        href = link_tag["href"]
                        full_link = urljoin(base_url, '/news/' + href.lstrip('./'))
                        date_text = date_tag.get_text(strip=True)
                        
                        pub_date = datetime.strptime(date_text.split()[0], "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ì¤‘ë¶€ì‹œì‚¬ì‹ ë¬¸", "ë³´ë„ì œëª©": title, "ë§í¬": full_link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [ì¤‘ë¶€ì‹œì‚¬] íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue
                
                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ì¤‘ë¶€ì‹œì‚¬ì‹ ë¬¸",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ì¤‘ë¶€ì‹œì‚¬] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ì¤‘ë¶€ì‹œì‚¬ì‹ ë¬¸",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ì¤‘ë¶€ì‹œì‚¬] ê¸°ì‚¬ ë¡œë”© ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ì¤‘ë¶€ì‹œì‚¬] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_portalnews.py (ìµœì í™”)
def fetch_portalnews_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.portalnews.co.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            search_url = f"{base_url}/news/search_result.html?search_mode=multi&s_title=1&s_body=1&search={quote(keyword)}&s_sdate=&s_edate=&catno=106"
            print(f"ğŸŒ [í¬íƒˆë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.art_list_all li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                items = soup.select("ul.art_list_all li")
                found_count = 0
                no_date_count = 0 
                
                for item in items:
                    try:
                        title_tag = item.select_one("h2")
                        link_tag = item.select_one("a")
                        date_tag = item.select_one("li.date")
                        
                        if not title_tag or not link_tag: continue
                        
                        title = title_tag.text.strip()
                        link = link_tag.get("href", "")
                        
                        if not link.startswith("http"):
                            link = base_url + link if link.startswith("/") else base_url + "/news/" + link
                            
                        date_str = "ê¸°ì‚¬ì—ì„œ ì§ì ‘ í™•ì¸ ë°”ëŒ"
                        pub_date = None
                        
                        if date_tag:
                            date_text = date_tag.text.strip()
                            try:
                                pub_date = datetime.strptime(date_text.split()[0], "%Y.%m.%d")
                                date_str = pub_date.strftime("%Y-%m-%d")
                            except:
                                pass 
                        
                        if pub_date and pub_date < date_limit: continue
                        
                        if not pub_date:
                            no_date_count += 1
                            if no_date_count > 5 and len(keywords) > 1: continue 
                        
                        # [v5.0] 'ê¸°íƒ€ì§€ë°©ì§€'ëŠ” ë‚ ì§œ ë¶ˆëª…í™• ì‹œ 'ê¸°ì‚¬ì—ì„œ ì§ì ‘ í™•ì¸ ë°”ëŒ'ìœ¼ë¡œ í‘œê¸°
                        results.append({"ë³´ë„ì¼": date_str, "ë³´ë„ë§¤ì²´": "í¬íƒˆë‰´ìŠ¤í†µì‹ ", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                        found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [í¬íƒˆë‰´ìŠ¤] ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "í¬íƒˆë‰´ìŠ¤í†µì‹ ",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [í¬íƒˆë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "í¬íƒˆë‰´ìŠ¤í†µì‹ ",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [í¬íƒˆë‰´ìŠ¤] í˜ì´ì§€ ë¡œë”© ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [í¬íƒˆë‰´ìŠ¤] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_kgyonhap.py (ìµœì í™”)
def fetch_kgyonhap_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://kgyonhapnews.net"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword)
            search_url = f"{base_url}/search.html?submit=submit&search_and=1&search_exec=all&search_section=all&news_order=1&search={encoded_keyword}&imageField.x=0&imageField.y=0"
            print(f"ğŸŒ [ê²½ê¸°ì—°í•©] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.search_result_list_box dl"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                found_count = 0
                
                for dl in soup.select("div.search_result_list_box dl"):
                    try:
                        a_tag = dl.select_one("dt > a")
                        if not a_tag: continue
                        
                        title = a_tag.get_text(strip=True)
                        link = a_tag["href"]
                        if not link.startswith("http"):
                            link = base_url + link
                            
                        date_tag = dl.select_one("dd.etc")
                        if not date_tag: continue
                        
                        date_match = re.search(r"\d{4}\.\d{2}\.\d{2}", date_tag.get_text())
                        if not date_match: continue
                        
                        pub_date = datetime.strptime(date_match.group(), "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ê²½ê¸°ì—°í•©ë‰´ìŠ¤", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [ê²½ê¸°ì—°í•©] ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ê²½ê¸°ì—°í•©ë‰´ìŠ¤",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ê²½ê¸°ì—°í•©] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ê²½ê¸°ì—°í•©ë‰´ìŠ¤",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ê²½ê¸°ì—°í•©] í˜ì´ì§€ ë¡œë”© ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ê²½ê¸°ì—°í•©] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_kocus.py (ìµœì í™”)
def fetch_kocus_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "http://www.kocus.com"
    search_path = "/news/articleList.html"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            params = {
                "sc_area": "A", "sc_word": keyword, "view_type": ""
            }
            # EUC-KR ì¸ì½”ë”©ì´ í•„ìš”í•¨
            full_url = f"{base_url}{search_path}?{urlencode(params, encoding='euc-kr')}"
            print(f"ğŸŒ [êµì°¨ë¡œì €ë„] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {full_url})")

            driver.get(full_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td.list-titles"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                title_links = soup.select("td.list-titles a")
                found_count = 0
                no_date_count = 0
                
                for link_tag in title_links:
                    try:
                        title = link_tag.text.strip()
                        article_url = urljoin(full_url, link_tag.get("href", ""))
                        row = link_tag.find_parent("tr")
                        date_cell = row.find("td", class_="list-times") if row else None
                        date_text = date_cell.text.strip() if date_cell else ""
                        
                        date_str = "ê¸°ì‚¬ì—ì„œ ì§ì ‘ í™•ì¸ ë°”ëŒ"
                        pub_date = None
                        
                        try:
                            pub_date = datetime.strptime(date_text, "%Y-%m-%d %H:%M")
                        except Exception:
                            try:
                                pub_date = datetime.strptime(date_text, "%Y.%m.%d")
                            except Exception:
                                pass
                                
                        if pub_date:
                            if pub_date < date_limit: continue
                            date_str = pub_date.strftime("%Y-%m-%d")
                        else:
                            no_date_count += 1
                            if no_date_count > 5 and len(keywords) > 1: continue 

                        results.append({"ë³´ë„ì¼": date_str, "ë³´ë„ë§¤ì²´": "êµì°¨ë¡œì €ë„", "ë³´ë„ì œëª©": title, "ë§í¬": article_url, "ê²€ìƒ‰ì–´": keyword})
                        found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [êµì°¨ë¡œì €ë„] ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "êµì°¨ë¡œì €ë„",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": full_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [êµì°¨ë¡œì €ë„] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "êµì°¨ë¡œì €ë„",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": full_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âš ï¸ [êµì°¨ë¡œì €ë„] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [êµì°¨ë¡œì €ë„] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_vision21.py (ìµœì í™”)
def fetch_vision21_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https.www.vision21.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            params = {"search": keyword}
            search_url = f"{base_url}/news/search_result.html?{urlencode(params, encoding='utf-8')}"
            print(f"ğŸŒ [ë¹„ì „21ë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)
            
            try:
                try:
                    WebDriverWait(driver, 1).until(EC.alert_is_present())
                    alert = driver.switch_to.alert
                    alert_text = alert.text
                    alert.accept()
                    print(f"âŒ [ë¹„ì „21ë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ì–¼ëŸ¿ ë°œìƒ: {alert_text}")
                    if "ê²€ìƒ‰ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in alert_text:
                        raise TimeoutException("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (Alert)")
                except TimeoutException as e:
                    if "Alert" in str(e): raise 
                    pass 

                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.art_list_all li"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("ul.art_list_all > li")
                found_count = 0
                no_date_count = 0
                
                for article in articles:
                    try:
                        title_tag = article.select_one("h2.cmp.c2") or article.select_one("h4.cmp.c2")
                        link_tag = article.select_one("a[href*='article.html']")
                        date_tag = article.select_one("li.date")
                        
                        if not title_tag or not link_tag: continue
                        
                        title = title_tag.get_text(strip=True)
                        href = link_tag.get("href", "")
                        article_url = urljoin(base_url + "/news/", href)
                        date_text = date_tag.get_text(strip=True) if date_tag else ""
                        
                        date_str = "ê¸°ì‚¬ì—ì„œ ì§ì ‘ í™•ì¸ ë°”ëŒ"
                        pub_date = None
                        
                        if date_text:
                            date_str_clean = date_text.split()[0].replace('-', '.')
                            try:
                                pub_date = datetime.strptime(date_str_clean, "%Y.%m.%d")
                                date_str = pub_date.strftime("%Y-%m-%d")
                            except Exception as pe:
                                pass

                        if pub_date and pub_date < date_limit: continue
                        
                        if not pub_date:
                            no_date_count += 1
                            if no_date_count > 5 and len(keywords) > 1: continue 

                        results.append({"ë³´ë„ì¼": date_str, "ë³´ë„ë§¤ì²´": "ë¹„ì „21ë‰´ìŠ¤", "ë³´ë„ì œëª©": title, "ë§í¬": article_url, "ê²€ìƒ‰ì–´": keyword})
                        found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [ë¹„ì „21ë‰´ìŠ¤] ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ë¹„ì „21ë‰´ìŠ¤",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ë¹„ì „21ë‰´ìŠ¤] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ/Alert)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ë¹„ì „21ë‰´ìŠ¤",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ë¹„ì „21ë‰´ìŠ¤] ì „ì²´ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ë¹„ì „21ë‰´ìŠ¤] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_newstoday24.py (ìµœì í™”)
def fetch_newstoday24_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://www.newstoday.or.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            search_url = f"{base_url}/news/articleList.html?sc_area=A&view_type=sm&sc_word={quote(keyword)}"
            print(f"ğŸŒ [ë‰´ìŠ¤íˆ¬ë°ì´24] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.view-cont"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                article_blocks = soup.select("div.view-cont")
                found_count = 0
                
                for block in article_blocks:
                    try:
                        title_tag = block.select_one("h4.titles a")
                        date_tag = block.select_one("em.replace-date")
                        
                        if not title_tag or not date_tag: continue
                        
                        title = title_tag.get_text(strip=True)
                        href = title_tag.get("href", "")
                        link = f"{base_url}{href}" if href.startswith("/") else href
                        date_text_raw = date_tag.get_text(strip=True)
                        
                        if not date_text_raw: continue
                        date_text = date_text_raw.split(" ")[0]
                        
                        pub_date = datetime.strptime(date_text, "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ë‰´ìŠ¤íˆ¬ë°ì´24", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [ë‰´ìŠ¤íˆ¬ë°ì´24] íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ë‰´ìŠ¤íˆ¬ë°ì´24",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ë‰´ìŠ¤íˆ¬ë°ì´24] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ë‰´ìŠ¤íˆ¬ë°ì´24",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"ğŸ” [ë‰´ìŠ¤íˆ¬ë°ì´24] ê¸°ì‚¬ ë¡œë”© ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ë‰´ìŠ¤íˆ¬ë°ì´24] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_tgh.py (ìµœì í™”)
def fetch_tgh_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "http://www.tgh.kr"
    search_path = "/news/articleList.html"
    driver = setup_driver_compatible(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            params = {
                "sc_area": "A", "sc_word": keyword, "sc_order_by": "E", "view_type": ""
            }
            full_url = f"{base_url}{search_path}?{urlencode(params, encoding='euc-kr')}"
            print(f"ğŸŒ [íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {full_url})")

            driver.get(full_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td.list-titles a"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                title_links = soup.select("td.list-titles a")
                found_count = 0
                no_date_count = 0
                
                for link_tag in title_links:
                    try:
                        title = link_tag.get_text(strip=True)
                        article_url = urljoin(full_url, link_tag.get("href", ""))
                        row = link_tag.find_parent("tr")
                        date_cell = row.find("td", class_="list-times") if row else None
                        
                        if not date_cell: continue
                        
                        date_text = date_cell.get_text(strip=True)
                        date_str = "ê¸°ì‚¬ì—ì„œ ì§ì ‘ í™•ì¸ ë°”ëŒ"
                        pub_date = None
                        
                        if date_text.strip():
                            try:
                                pub_date = datetime.strptime(date_text, "%Y-%m-%d %H:%M")
                                date_str = pub_date.strftime("%Y-%m-%d")
                            except Exception:
                                pass
                                
                        if pub_date and pub_date < date_limit: continue
                        
                        if not pub_date:
                            no_date_count += 1
                            if no_date_count > 5 and len(keywords) > 1: continue 

                        results.append({"ë³´ë„ì¼": date_str, "ë³´ë„ë§¤ì²´": "íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨", "ë³´ë„ì œëª©": title, "ë§í¬": article_url, "ê²€ìƒ‰ì–´": keyword})
                        found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨] ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´ ë§í¬ í™•ì¸)",
                        "ë§í¬": full_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨",
                    "ë³´ë„ì œëª©": f"ì›¹ì‚¬ì´íŠ¸ ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤íŒ¨ (í‚¤ì›Œë“œ: '{keyword}')",
                    "ë§í¬": full_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âš ï¸ [íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_mediareport.py (ìµœì í™”)
def fetch_mediareport_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "https://mediareport.co.kr"
    driver = setup_driver(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword)
            search_url = f"{base_url}/search.html?submit=submit&search={encoded_keyword}&search_and=2&search_exec=all&search_section=all&news_order=1"
            print(f"ğŸŒ [ë¯¸ë””ì–´ë¦¬í¬íŠ¸] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.search_result_list_box dl"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                found_count = 0
                
                for dl_tag in soup.select("div.search_result_list_box dl"):
                    try:
                        dt_tag = dl_tag.find("dt")
                        if not dt_tag: continue
                        a_tag = dt_tag.find("a")
                        if not a_tag: continue
                        
                        title = a_tag.get_text(strip=True)
                        link = a_tag["href"]
                        if not link.startswith("http"):
                            link = base_url + link
                            
                        date_tag = dl_tag.find("dd", class_="etc")
                        if not date_tag: continue
                        
                        date_match = re.search(r"\d{4}\.\d{2}\.\d{2}", date_tag.get_text())
                        if not date_match: continue
                        
                        pub_date = datetime.strptime(date_match.group(), "%Y.%m.%d")
                        
                        if pub_date >= date_limit:
                            results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ë¯¸ë””ì–´ë¦¬í¬íŠ¸", "ë³´ë„ì œëª©": title, "ë§í¬": link, "ê²€ìƒ‰ì–´": keyword})
                            found_count += 1
                            
                    except Exception as e_item:
                        print(f"âš ï¸ [ë¯¸ë””ì–´ë¦¬í¬íŠ¸] ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ë¯¸ë””ì–´ë¦¬í¬íŠ¸",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ë¯¸ë””ì–´ë¦¬í¬íŠ¸] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ë¯¸ë””ì–´ë¦¬í¬íŠ¸",
                    "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ë¯¸ë””ì–´ë¦¬í¬íŠ¸] í˜ì´ì§€ ë¡œë”© ì˜¤ë¥˜: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ë¯¸ë””ì–´ë¦¬í¬íŠ¸] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# fetch_sudokwon.py (ìµœì í™”)
def fetch_sudokwon_multi(keywords, date_limit, days_limit):
    results = []
    base_url = "http://www.sudokwon.com"
    driver = setup_driver_compatible(headless=True)
    if not driver: return []
    
    try:
        for keyword in keywords:
            encoded_keyword = quote(keyword.encode('euc-kr'))
            search_url = f"{base_url}/searchs.php?searchword={encoded_keyword}&x=15&y=20"
            print(f"ğŸŒ [ìˆ˜ë„ê¶Œì¼ë³´] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {search_url})")

            driver.get(search_url)

            try:
                # âœ… [ìˆ˜ì •] ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆì—ì„œ 10ì´ˆë¡œ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.
                WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.sublist"))
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                articles = soup.select("td[style*='padding:10px']")
                found_count = 0
                
                for article_td in articles:
                    try:
                        title_tag = article_td.select_one("a.sublist")
                        date_tag = article_td.select_one("span.date")
                        
                        if not (title_tag and date_tag): continue
                        
                        title = title_tag.get_text(strip=True)
                        href = title_tag.get("href", "")
                        full_link = f"{base_url}/{href.lstrip('/')}"
                        date_text = date_tag.get_text(strip=True)
                        
                        if re.match(r"\d{4}\.\s?\d{2}\.\s?\d{2}", date_text):
                            pub_date = datetime.strptime(date_text.replace(" ", ""), "%Y.%m.%d")
                            if pub_date >= date_limit:
                                results.append({"ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"), "ë³´ë„ë§¤ì²´": "ìˆ˜ë„ê¶Œì¼ë³´", "ë³´ë„ì œëª©": title, "ë§í¬": full_link, "ê²€ìƒ‰ì–´": keyword})
                                found_count += 1
                        
                    except Exception as e_item:
                        print(f"âš ï¸ [ìˆ˜ë„ê¶Œì¼ë³´] íŒŒì‹± ì˜ˆì™¸: {e_item} (í‚¤ì›Œë“œ: {keyword})")
                        continue

                if found_count == 0:
                    results.append({
                        "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                        "ë³´ë„ë§¤ì²´": "ìˆ˜ë„ê¶Œì¼ë³´",
                        "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´ ë§í¬ í™•ì¸)",
                        "ë§í¬": search_url,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    
            except TimeoutException:
                print(f"ğŸ” [ìˆ˜ë„ê¶Œì¼ë³´] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (íƒ€ì„ì•„ì›ƒ - 10ì´ˆ ëŒ€ê¸°)")
                results.append({
                    "ë³´ë„ì¼": f"{days_limit}ì¼ ì´ë‚´ ì—†ìŒ",
                    "ë³´ë„ë§¤ì²´": "ìˆ˜ë„ê¶Œì¼ë³´",
                    "ë³´ë„ì œëª©": f"ì›¹ì‚¬ì´íŠ¸ ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤íŒ¨ (í‚¤ì›Œë“œ: '{keyword}')",
                    "ë§í¬": search_url,
                    "ê²€ìƒ‰ì–´": keyword
                })
                
    except Exception as e_page:
        print(f"âŒ [ìˆ˜ë„ê¶Œì¼ë³´] ê¸°ì‚¬ ë¡œë”© ì‹¤íŒ¨: {e_page}")
    finally:
        if driver:
            try: driver.quit()
            except Exception as e_quit: print(f"âš ï¸ [ìˆ˜ë„ê¶Œì¼ë³´] ë“œë¼ì´ë²„ ì¢…ë£Œ ì˜ˆì™¸: {e_quit}")
    return results

# =============================================================================
# 6. [v5.0] Fetcher í•¨ìˆ˜ ê·¸ë£¹ 3 (Google ë‰´ìŠ¤)
# (ì´í•˜ v5.0 ì›ë³¸ê³¼ ë™ì¼ ... fetch_google_news)
# =============================================================================

def shorten_google_url(url):
    """êµ¬ê¸€ ë¦¬ë””ë ‰ì…˜ URLì—ì„œ ì‹¤ì œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if "google.com/url?" in url:
        try:
            # URLì˜ ì¿¼ë¦¬ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ 'url' íŒŒë¼ë¯¸í„° ê°’ì„ ì¶”ì¶œ
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            if 'url' in query_params:
                return query_params['url'][0]
        except Exception as e:
            print(f"URL ë‹¨ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (URL: {url})")
            return url # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ URL ë°˜í™˜
    # êµ¬ê¸€ ë¦¬ë””ë ‰ì…˜ URLì´ ì•„ë‹ˆë©´ ì›ë³¸ URL ë°˜í™˜
    return url

def fetch_google_news_feed(keywords, date_limit, days_limit):
    """
    Google News RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    âœ… [v5.0.3 ìˆ˜ì •] Seleniumì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… URLì„ í™•ì¸í•©ë‹ˆë‹¤. (ëŠë¦´ ìˆ˜ ìˆìŒ)
    """
    source_name = "Googleë‰´ìŠ¤"
    results = []
    
    found_for_keyword = {keyword: False for keyword in keywords}

    driver = None
    try:
        # í˜¸í™˜ì„± ë“œë¼ì´ë²„(ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ ì—†ìŒ)ë¥¼ ì‚¬ìš©í•´ì•¼ ë¦¬ë””ë ‰ì…˜ì´ ì˜ ë©ë‹ˆë‹¤.
        driver = setup_driver_compatible(headless=True)
        if not driver:
            print("â€¼ï¸ [Googleë‰´ìŠ¤] Selenium ë“œë¼ì´ë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ì–´ URL ë‹¨ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"â€¼ï¸ [Googleë‰´ìŠ¤] Selenium ë“œë¼ì´ë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")

    for keyword in keywords:
        # Google News RSS í”¼ë“œ URL ìƒì„±
        # âœ… [v5.0.1] 'ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­' OR 'ê´‘ì£¼í•˜ë‚¨' -> "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­" í†µì¼
        search_keyword = keyword
        if keyword == "ê´‘ì£¼í•˜ë‚¨":
            search_keyword = "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­"
        
        # âœ… [v5.0.1] ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œì— í°ë”°ì˜´í‘œ ì¶”ê°€
        query = f'"{search_keyword}"'
        
        # âœ… [v5.0.1] daterange ëŒ€ì‹  after/before ì‚¬ìš© (ë” ì•ˆì •ì )
        # ex: "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­" after:2024-05-20 before:2024-05-27
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_limit)
        query += f" after:{start_date.strftime('%Y-%m-%d')} before:{end_date.strftime('%Y-%m-%d')}"

        # RSS í”¼ë“œ URL
        url = f"https://news.google.com/rss/search?q={quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        print(f"ğŸŒ [{source_name}] '{keyword}' ê²€ìƒ‰ ì¤‘... (URL: {url})")

        try:
            feed = feedparser.parse(url)
            
            if not feed.entries:
                # âœ… [v5.0.1] 'ì—†ìŒ' ë©”ì‹œì§€ëŠ” ë§ˆì§€ë§‰ì— í•œë²ˆë§Œ ì¶”ê°€
                continue

            for entry in feed.entries:
                try:
                    # --- ë‚ ì§œ íŒŒì‹± ---
                    pub_date_parsed = entry.get("published_parsed")
                    if not pub_date_parsed:
                        continue
                    
                    pub_date = datetime.fromtimestamp(time.mktime(pub_date_parsed))
                    
                    # --- ê¸°ê°„ í•„í„°ë§ ---
                    if pub_date < date_limit:
                        continue

                    # --- ì œëª© ë° ë§í¬ íŒŒì‹± ---
                    title = entry.title
                    original_link = entry.link
                    final_link = original_link

                    # âœ… [v5.0.3 ìˆ˜ì •] Seleniumìœ¼ë¡œ ë§í¬ ë‹¨ì¶•
                    if driver and "news.google.com/rss/articles/" in original_link:
                        try:
                            driver.set_page_load_timeout(15) # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                            driver.get(original_link)
                            # URLì´ ë°”ë€Œê³ , ë” ì´ìƒ êµ¬ê¸€ì´ ì•„ë‹ ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
                            WebDriverWait(driver, 10).until(
                                lambda d: d.current_url != original_link and "google.com" not in d.current_url
                            )
                            final_link = driver.current_url
                            print(f"  [ë§í¬ ë‹¨ì¶•] ì„±ê³µ: {final_link[:70]}...")
                        except TimeoutException:
                            final_link = driver.current_url 
                            print(f"  [ë§í¬ ë‹¨ì¶•] íƒ€ì„ì•„ì›ƒ. í˜„ì¬ URL ì‚¬ìš©: {final_link[:70]}...")
                        except Exception as e:
                            print(f"  [ë§í¬ ë‹¨ì¶•] ì˜¤ë¥˜: {e}")
                            final_link = original_link # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë§í¬ ì‚¬ìš©
                    else:
                        # Selenium ë“œë¼ì´ë²„ê°€ ì—†ê±°ë‚˜ ì¼ë°˜ ë§í¬ì¼ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                        final_link = shorten_google_url(original_link)


                    # --- ê²°ê³¼ ì¶”ê°€ ---
                    results.append({
                        "ë³´ë„ì¼": pub_date.strftime("%Y-%m-%d"),
                        "ë³´ë„ë§¤ì²´": source_name,
                        "ë³´ë„ì œëª©": title,
                        "ë§í¬": final_link,
                        "ê²€ìƒ‰ì–´": keyword
                    })
                    found_for_keyword[keyword] = True

                except Exception as e:
                    print(f"[{source_name}] ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e} (ì œëª©: {entry.get('title', 'N/A')})")
                    continue
        
        except Exception as e:
            print(f"[{source_name}] '{keyword}' í”¼ë“œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            # âœ… [v5.0.1] 'ì—†ìŒ' ë©”ì‹œì§€ëŠ” ë§ˆì§€ë§‰ì— í•œë²ˆë§Œ ì¶”ê°€
            continue

    # âœ… [v5.0.3 ìˆ˜ì •] ë“œë¼ì´ë²„ ì¢…ë£Œ
    if driver:
        driver.quit()

    # âœ… [v5.0.1] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” í‚¤ì›Œë“œì— ëŒ€í•´ 'ì—†ìŒ' ë©”ì‹œì§€ ì¶”ê°€
    for keyword, found in found_for_keyword.items():
        if not found:
            # 'ì—†ìŒ' ë©”ì‹œì§€ì— ëŒ€í•œ URLì€ ì‹¤ì œ ê²€ìƒ‰í–ˆë˜ RSS URLì„ ì œê³µ
            search_keyword = keyword
            if keyword == "ê´‘ì£¼í•˜ë‚¨":
                search_keyword = "ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­"
            query = f'"{search_keyword}"'
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_limit)
            query += f" after:{start_date.strftime('%Y-%m-%d')} before:{end_date.strftime('%Y-%m-%d')}"
            url = f"https://news.google.com/rss/search?q={quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
            
            results.append({
                "ë³´ë„ì¼": "ì—†ìŒ",
                "ë³´ë„ë§¤ì²´": source_name,
                "ë³´ë„ì œëª©": f"ìµœê·¼ {days_limit}ì¼ ì´ë‚´ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "ë§í¬": url,
                "ê²€ìƒ‰ì–´": keyword
            })
            
    return results

# =============================================================================
# 7. [v5.0.1 ìˆ˜ì •] Tkinter GUI ì• í”Œë¦¬ì¼€ì´ì…˜ (í†µí•© ë° ìˆ˜ì •)
# =============================================================================

class NewsScraperApp:
    def __init__(self, root):
        self.root = root
        # âœ… [v5.0.1] GUI íƒ€ì´í‹€ ë³€ê²½ (ë³´ë„ìë£Œ ë·°ì–´ ê¸°ëŠ¥ í†µí•©)
        self.root.title("í†µí•© ë‰´ìŠ¤ ìŠ¤í¬ë ˆì´í¼ v5.0.1 (ë³´ë„ìë£Œ ì—‘ì…€ ì €ì¥ ê¸°ëŠ¥ í¬í•¨)") 
        
        # [v5.0] GUI í¬ê¸° (16ëŒ€ ì§€ë°©ì§€ ê¸°ì¤€ 1125x750) -> 1125x800 (ì²´í¬ë°•ìŠ¤ ê³µê°„)
        self.root.geometry("1125x800") 

        self.goegh_base_url = "https://www.goegh.kr/goegh/na/ntt/selectNttList.do?mi=8686&bbsId=5041"
        self.goegh_page1_url = self.goegh_base_url + "&pageIndex=1"

        self.earliest_date_clicked = None

        # âœ… [v5.0.1 ì‹ ê·œ] ì—‘ì…€ ì €ì¥ì„ ìœ„í•œ ì›ë³¸ ë°ì´í„° ì €ì¥ì†Œ (v1.4 ê¸°ì¤€)
        self.tree_data = {}
        # âœ… [Word ì €ì¥] ìµœì‹  í†µí•© ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.latest_articles_df = None

        self.style = ttk.Style()
        self.style.configure('.', font=('Malgun Gothic', 12))
        self.style.configure('TLabelframe.Label', font=('Malgun Gothic', 12, 'bold'))
        self.style.configure('Status.TLabel', font=('Malgun Gothic', 11))
        self.style.configure('Treeview.Heading', font=('Malgun Gothic', 11, 'bold'))
        # [v5.0] '6ëŒ€ ì§€ë°©ì§€'ì˜ Treeview í°íŠ¸(12) ë° rowheight ì ìš©
        self.style.configure('Treeview', font=('Malgun Gothic', 12), rowheight=int(12 * 2.2))

        # âœ… (v1.4) [ìš”ì²­ 3] ê°•ì¡° ë²„íŠ¼ ìŠ¤íƒ€ì¼ ì¶”ê°€
        self.style.configure('Emph.TButton', font=('Malgun Gothic', 12, 'bold'), foreground='#0000AA')

        main_frame = ttk.Frame(root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)

        # --- 1. [ì°¸ê³ ] í”„ë ˆì„ ---
        goegh_frame = ttk.LabelFrame(main_frame, text=" [ì°¸ê³ ] ê´‘ì£¼í•˜ë‚¨êµìœ¡ì§€ì›ì²­ ë³´ë„ìë£Œ ", padding="10")
        goegh_frame.pack(fill=tk.X, expand=False, pady=(0, 5))
        link_frame = ttk.Frame(goegh_frame)
        link_frame.pack(fill=tk.X)
        link_label = ttk.Label(link_frame, text="â–¶ ë³´ë„ë°°í¬(ì„±ê³¼íŒ€->ì–¸ë¡ ) ë§í¬ (í´ë¦­)",
                               foreground="blue", cursor="hand2", font=('Malgun Gothic', 12, 'underline'))
        link_label.pack(side=tk.LEFT, padx=5)
        link_label.bind("<Button-1>", self.open_link)

        # --- 2. [Step 1] ë³´ë„ìë£Œ ëª©ë¡ í”„ë ˆì„ (v5.0.1 ìˆ˜ì •) ---
        press_release_frame = ttk.LabelFrame(main_frame, text=" [Step 1] ë³´ë„ìë£Œ ëª©ë¡ í™•ì¸ (ë”ë¸”í´ë¦­ ì‹œ ê²€ìƒ‰ì–´ ëˆ„ì ) ", padding="10")
        press_release_frame.pack(fill=tk.BOTH, expand=True, pady=5)

        # âœ… [v5.0.1 ìˆ˜ì •] v1.4ì˜ ë²„íŠ¼ í”„ë ˆì„ ë ˆì´ì•„ì›ƒ ì ìš©
        button_frame = ttk.Frame(press_release_frame)
        button_frame.pack(fill=tk.X, pady=(0, 10))

        # (v1.4) [ìˆ˜ì •] (1) ì „ì²´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° - command ë³€ê²½
        self.fetch_press_button = ttk.Button(button_frame, text="ë³´ë„ë°°í¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°(1í˜ì´ì§€)", command=self.start_fetch_press_releases_all)
        self.fetch_press_button.pack(side=tk.LEFT, ipady=3, padx=(0, 10))
        
        # âœ… (v1.4) [ì‹ ê·œ] (2) ì˜¤ëŠ˜ ë‚ ì§œ ë²„íŠ¼ ì¶”ê°€
        today_str = datetime.now().strftime('%Y-%m-%d')
        self.fetch_today_button = ttk.Button(
            button_frame, 
            text=f"'{today_str}' ì¼ì ëª©ë¡ ê°€ì ¸ì˜¤ê¸°", 
            command=self.start_fetch_press_releases_today,
            style='Emph.TButton' # ê°•ì¡° ìŠ¤íƒ€ì¼ ì ìš©
        )
        self.fetch_today_button.pack(side=tk.LEFT, ipady=3, padx=(5, 10))

        # âœ… (v1.4) [ì‹ ê·œ] (3) ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
        self.export_button = ttk.Button(button_frame, text="ì—‘ì…€íŒŒì¼ë¡œ ë‹¤ìš´", command=self.export_to_excel)
        self.export_button.pack(side=tk.LEFT, ipady=3, padx=(10, 0))
        self.export_button.config(state=tk.DISABLED)

        tree_frame = ttk.Frame(press_release_frame)
        tree_frame.pack(fill=tk.BOTH, expand=True)
        scrollbar = ttk.Scrollbar(tree_frame, orient=tk.VERTICAL)
        
        # âœ… [v5.0.1] GUI TreeViewëŠ” v5.0ì˜ (Title, Date, Keyword) 3ë‹¨ ì»¬ëŸ¼ì„ ìœ ì§€
        self.press_tree = ttk.Treeview(tree_frame, columns=("Title", "Date", "Keyword"), show="headings", yscrollcommand=scrollbar.set, height=7)
        scrollbar.config(command=self.press_tree.yview)

        self.press_tree.heading("Title", text="ë³´ë„ì œëª©")
        self.press_tree.heading("Date", text="ë“±ë¡ì¼")
        self.press_tree.heading("Keyword", text="ê²€ìƒ‰ì–´ ì¶”ì²œ")
        
        self.press_tree.column("Title", width=800, anchor=tk.W)
        self.press_tree.column("Date", width=100, anchor=tk.CENTER)
        self.press_tree.column("Keyword", width=120, anchor=tk.CENTER)

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.press_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        
        # âœ… [v5.0.1] ë”ë¸”í´ë¦­ ì´ë²¤íŠ¸ëŠ” v5.0ì˜ ê²ƒì„ ìœ ì§€ (ê²€ìƒ‰ì–´ ëˆ„ì  ê¸°ëŠ¥)
        self.press_tree.bind("<Double-1>", self.on_press_release_double_click)


        # --- 3. [Step 2] í†µí•© ê²€ìƒ‰ í”„ë ˆì„ ---
        # (ì´í•˜ v5.0 ì›ë³¸ê³¼ ë™ì¼)
        scraper_frame = ttk.LabelFrame(main_frame, text=" [Step 2] í†µí•© ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ ", padding="10")
        scraper_frame.pack(fill=tk.X, expand=False, pady=5) 

        # --- [v5.0] ê²€ìƒ‰ì–´ ë° Nì¼ (ê¸°ì¡´ê³¼ ë™ì¼) ---
        ttk.Label(scraper_frame, text="ê²€ìƒ‰ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„):").grid(row=0, column=0, padx=5, pady=5, sticky=tk.W)
        
        self.keyword_text_var = tk.StringVar()
        self.keywords_entry = ttk.Entry(scraper_frame, width=60, textvariable=self.keyword_text_var)
        self.keywords_entry.grid(row=0, column=1, padx=5, pady=5, sticky=tk.EW)
        self.keyword_text_var.trace_add("write", self.on_keyword_entry_change)
        
        ttk.Label(scraper_frame, text="ìµœê·¼ Nì¼ (ìˆ«ì):").grid(row=1, column=0, padx=5, pady=5, sticky=tk.W)
        self.days_entry = ttk.Entry(scraper_frame, width=10)
        self.days_entry.grid(row=1, column=1, padx=5, pady=5, sticky=tk.W)
        
        # --- âœ… [v5.0] ê²€ìƒ‰ ëŒ€ìƒ ì²´í¬ë°•ìŠ¤ í”„ë ˆì„ (ì‹ ê·œ) ---
        check_frame = ttk.LabelFrame(scraper_frame, text=" ê²€ìƒ‰ ëŒ€ìƒ (ì¤‘ë³µ ì„ íƒ ê°€ëŠ¥) ", padding="5")
        check_frame.grid(row=2, column=0, columnspan=2, padx=5, pady=(5, 10), sticky=tk.EW)
        
        self.fetch_main6_var = tk.BooleanVar(value=True)
        self.fetch_other16_var = tk.BooleanVar(value=True)
        self.fetch_google_var = tk.BooleanVar(value=True)
        
        ttk.Checkbutton(check_frame, text="ì£¼ìš” 6ëŒ€ ì§€ë°©ì§€ (6ê°œ)", variable=self.fetch_main6_var).pack(side=tk.LEFT, padx=10)
        ttk.Checkbutton(check_frame, text="ê¸°íƒ€ ì§€ë°©ì§€ (16ê°œ)", variable=self.fetch_other16_var).pack(side=tk.LEFT, padx=10)
        ttk.Checkbutton(check_frame, text="Google ë‰´ìŠ¤ (RSS)", variable=self.fetch_google_var).pack(side=tk.LEFT, padx=10)

        # --- âœ… [v5.0] ì‹œì‘/Word ì €ì¥ ë²„íŠ¼ ì˜ì—­ ---
        button_frame = ttk.Frame(scraper_frame)
        button_frame.grid(row=0, column=2, rowspan=3, padx=10, pady=5, sticky=tk.NS)

        self.start_button = ttk.Button(button_frame, text="í†µí•© ê²€ìƒ‰ ë° ì—‘ì…€ ì €ì¥", command=self.start_main_scraper_thread)
        self.start_button.pack(fill=tk.BOTH, expand=True, ipadx=5, ipady=8, pady=(0, 5))

        self.word_button = ttk.Button(button_frame, text="Word ë¬¸ì„œë¡œ ì €ì¥",
                                      command=self.start_word_export_thread,
                                      state=tk.DISABLED)
        self.word_button.pack(fill=tk.BOTH, expand=True, ipadx=5, ipady=8)

        self.start_button.bind("<Return>", lambda event: self.start_main_scraper_thread())
        
        scraper_frame.columnconfigure(1, weight=1)

        # --- ìƒíƒœë°” ---
        self.status_var = tk.StringVar()
        self.status_var.set("ì¤€ë¹„ ì™„ë£Œ (v5.0.1). [Step 1]ì—ì„œ ëª©ë¡ì„ ê°€ì ¸ì˜¤ê±°ë‚˜, [Step 2]ì— ê²€ìƒ‰ì–´ë¥¼ ë°”ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
        
        status_bar = ttk.Label(root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W, padding="5", style='Status.TLabel')
        status_bar.pack(side=tk.BOTTOM, fill=tk.X)
        # (ì´ì–´ì„œ __init__ ë©”ì„œë“œ ë)

    def open_link(self, event):
        self.status_var.set(f"{self.goegh_page1_url} ì£¼ì†Œë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        webbrowser.open_new(self.goegh_page1_url)

    def open_result_folder(self, filename):
        # [v5.0] '6ëŒ€' ê¸°ì¤€ (os.startfile) ë° 'ê¸°íƒ€' ê¸°ì¤€ (í”Œë«í¼ ë¶„ê¸°) í†µí•©
        try:
            folder_path = os.path.dirname(os.path.abspath(filename))
            if sys.platform == "win32":
                os.startfile(folder_path)
            elif sys.platform == "darwin": # macOS
                subprocess.Popen(["open", folder_path])
            else: # Linux
                subprocess.Popen(["xdg-open", folder_path])
            print(f"âœ… ê²°ê³¼ í´ë” ìë™ ì—´ê¸° ì‹œë„: {folder_path}")
        except Exception as e:
            print(f"â€¼ï¸ ê²°ê³¼ í´ë” ìë™ ì—´ê¸° ì‹¤íŒ¨: {e}")
            messagebox.showinfo("ì•Œë¦¼", f"ê²°ê³¼ ì—‘ì…€ íŒŒì¼ì€ í˜„ì¬ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n{filename}\nìˆ˜ë™ìœ¼ë¡œ í´ë”ë¥¼ ì—´ì–´ í™•ì¸í•´ì£¼ì„¸ìš”.")

    def open_file_directly(self, filename):
        try:
            abs_path = os.path.abspath(filename)
            if sys.platform == "win32":
                os.startfile(abs_path)
            elif sys.platform == "darwin":
                subprocess.Popen(["open", abs_path])
            else:
                subprocess.Popen(["xdg-open", abs_path])
            print(f"âœ… íŒŒì¼ ì—´ê¸° ì‹œë„: {abs_path}")
        except Exception as e:
            print(f"â€¼ï¸ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {e}")
            messagebox.showinfo("ì•Œë¦¼", f"íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n{filename}\nìˆ˜ë™ìœ¼ë¡œ ì—´ì–´ì£¼ì„¸ìš”.")

    def show_completion_window(self, filename, article_count):
        # [v5.0] '6ëŒ€ ì§€ë°©ì§€' ê¸°ì¤€ íŒì—… (í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì§€ ì•Šê³  í´ë”ë§Œ ì—´ê¸°)
        top = Toplevel(self.root)
        top.title("ì‘ì—… ì™„ë£Œ")
        top.geometry("350x180") 
        top.transient(self.root) 
        top.grab_set() 
        main_frame = ttk.Frame(top, padding="15")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        basename = os.path.basename(filename)

        # âœ… [v5.0.1] íŒì—… í…ìŠ¤íŠ¸ ë¶„ê¸° (í†µí•©ê²€ìƒ‰ / ë³´ë„ìë£Œ / Word)
        if basename.lower().endswith(".docx"):
            title_text = "âœ… Word ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
            count_text = f"íŒŒì¼ëª…: {basename}\n(ì´ {article_count}ê°œ ê¸°ì‚¬ ê¸°ë°˜)"
            open_label = "ë¬¸ì„œ ì—´ê¸°"
            open_command = lambda: [self.open_file_directly(filename), top.destroy()]
        elif "í†µí•©_ë‰´ìŠ¤ê²€ìƒ‰_" in basename:
            title_text = "âœ… ìŠ¤í¬ë˜í•‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            count_text = f"íŒŒì¼ëª…: {basename}\n(ì´ {article_count}ê°œ í•­ëª©)"
            open_label = "í™•ì¸ (í´ë” ì—´ê¸°)"
            open_command = lambda: [self.open_result_folder(filename), top.destroy()]
        else:
            title_text = "âœ… ì—‘ì…€ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            count_text = f"íŒŒì¼ëª…: {basename}\n(ì´ {article_count}ê°œ í•­ëª©)"
            open_label = "í™•ì¸ (í´ë” ì—´ê¸°)"
            open_command = lambda: [self.open_result_folder(filename), top.destroy()]

        ttk.Label(main_frame, text=title_text, font=('Malgun Gothic', 12, 'bold')).pack(pady=(5, 0))
        ttk.Label(main_frame, text=count_text).pack(pady=(5, 10))

        open_button = ttk.Button(main_frame, text=open_label, command=open_command)
        open_button.pack(pady=5, ipadx=10, ipady=5)
        
        close_button = ttk.Button(main_frame, text="ë‹«ê¸°", command=top.destroy)
        close_button.pack(pady=5)
        
        self.root.wait_window(top) 

    # --- [Step 1] ë³´ë„ìë£Œ ìŠ¤í¬ë ˆì´í¼ ìŠ¤ë ˆë“œ ê´€ë¦¬ (v5.0.1 ìˆ˜ì •) ---

    def on_keyword_entry_change(self, *args):
        # (v5.0 ì›ë³¸ê³¼ ë™ì¼)
        try:
            if not self.keyword_text_var.get():
                if self.earliest_date_clicked is not None:
                    print("ê²€ìƒ‰ì–´ ì°½ì´ ë¹„ì›Œì ¸ Nì¼ ê³„ì‚° ê¸°ì¤€ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                    self.earliest_date_clicked = None
                    self.days_entry.delete(0, tk.END) 
                    self.status_var.set("ê²€ìƒ‰ì–´ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ê°„ë„ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"on_keyword_entry_change ì˜¤ë¥˜: {e}")


    def on_press_release_double_click(self, event):
        # (v5.0 ì›ë³¸ê³¼ ë™ì¼ - GUIê°€ (Title, Date, Keyword) ê¸°ì¤€ì´ë¯€ë¡œ)
        try:
            item_id = self.press_tree.focus() 
            if not item_id: return
            values = self.press_tree.item(item_id, 'values')
            if not values: return
            
            column_id = self.press_tree.identify_column(event.x)
            
            clicked_date_str = values[1].strip()
            single_keyword_to_add = values[2].strip()
            
            keywords_to_process = [] 

            if column_id == '#2':
                self.status_var.set(f"'{clicked_date_str}'ì™€(ê³¼) ë™ì¼í•œ ë‚ ì§œì˜ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                keywords_set = set() 
                all_item_ids = self.press_tree.get_children()
                
                for an_item_id in all_item_ids:
                    item_values = self.press_tree.item(an_item_id, 'values')
                    if not item_values: continue
                    item_date = item_values[1].strip()
                    item_keyword = item_values[2].strip()
                    
                    if item_date == clicked_date_str and item_keyword:
                        keywords_set.add(item_keyword)
                
                keywords_to_process = list(keywords_set)
                
            else:
                if not single_keyword_to_add:
                    self.status_var.set("ì´ í•­ëª©ì—ëŠ” ì¶”ì²œ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return
                keywords_to_process = [single_keyword_to_add]

            if not keywords_to_process:
                self.status_var.set("ì¶”ê°€í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            current_text = self.keyword_text_var.get().strip()
            current_keywords = [kw.strip() for kw in current_text.split(',') if kw.strip()]
            
            final_keywords_to_add = []
            for kw in keywords_to_process:
                if kw not in current_keywords:
                    final_keywords_to_add.append(kw)

            if not final_keywords_to_add:
                self.status_var.set(f"ì„ íƒí•œ í‚¤ì›Œë“œê°€ ì´ë¯¸ ëª¨ë‘ ëª©ë¡ì— ìˆìŠµë‹ˆë‹¤.")
                return

            # [v5.0] 12ê°œ ì œí•œ ë¡œì§ (3ê°œ íŒŒì¼ ê³µí†µ)
            current_count = len(current_keywords)
            space_available = 12 - current_count
            show_limit_popup = False

            if len(final_keywords_to_add) > space_available:
                if space_available <= 0: 
                    keywords_to_actually_add = []
                else: 
                    keywords_to_actually_add = final_keywords_to_add[:space_available]
                show_limit_popup = True
            else:
                keywords_to_actually_add = final_keywords_to_add
                
            if not keywords_to_actually_add: 
                messagebox.showwarning("ì…ë ¥ ì´ˆê³¼", "ê²€ìƒ‰ì–´ê°€ ì´ë¯¸ 12ê°œì…ë‹ˆë‹¤. (ìµœëŒ€ 12ê°œ)") 
                self.status_var.set("ê²€ìƒ‰ì–´ 12ê°œ ì´ˆê³¼. ì¶”ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            elif show_limit_popup:
                messagebox.showwarning("ì…ë ¥ ì´ˆê³¼", "ê²€ìƒ‰ì–´ëŠ” ìµœëŒ€ 12ê°œê¹Œì§€ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (12ê°œê¹Œì§€ë§Œ ì¶”ê°€ë¨)")
                
            current_keywords.extend(keywords_to_actually_add)
            new_text = ", ".join(current_keywords)
            self.keyword_text_var.set(new_text)

            # 'ìµœê·¼ Nì¼' ìë™ ê³„ì‚°
            date_str = clicked_date_str 
            n_days_calculated = None
            try:
                clicked_date = datetime.strptime(date_str, "%Y.%m.%d")
                
                if self.earliest_date_clicked is None or clicked_date < self.earliest_date_clicked:
                    self.earliest_date_clicked = clicked_date
                    
                today = datetime.today()
                delta = today.date() - self.earliest_date_clicked.date()
                n_days = delta.days + 1
                
                if n_days <= 0:
                    n_days = 1
                
                self.days_entry.delete(0, tk.END)
                self.days_entry.insert(0, str(n_days))
                n_days_calculated = n_days

            except ValueError:
                print(f"Nì¼ ìë™ê³„ì‚° ì˜¤ë¥˜: ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ ({date_str})")
            except Exception as e_date:
                print(f"Nì¼ ìë™ê³„ì‚° ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e_date}")

            status_message = (f"í‚¤ì›Œë“œ {len(keywords_to_actually_add)}ê°œ ì¶”ê°€ (ì´ {len(current_keywords)}ê°œ).")
            if n_days_calculated:
                status_message += f" | ê¸°ê°„ {n_days_calculated}ì¼ ìë™ ì„¤ì •."
            else:
                status_message += " | ê¸°ê°„ ê³„ì‚° ì‹¤íŒ¨."
            self.status_var.set(status_message)
        
        except Exception as e:
            self.status_var.set(f"ê²€ìƒ‰ì–´ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            traceback.print_exc()

    # âœ… [v5.0.1 ì‹ ê·œ] (1) ì „ì²´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (v1.4)
    def start_fetch_press_releases_all(self):
        self.set_step1_buttons_state(tk.DISABLED, "ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        self.press_tree.delete(*self.press_tree.get_children())
        self.tree_data.clear() 
        self.status_var.set("ê´‘ì£¼í•˜ë‚¨ ë³´ë„ìë£Œ ëª©ë¡ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤ (1í˜ì´ì§€)")
        # filter_today=False ì „ë‹¬
        threading.Thread(target=self.run_fetch_press_releases_task, args=(False,), daemon=True).start()

    # âœ… [v5.0.1 ì‹ ê·œ] (2) ì˜¤ëŠ˜ ë‚ ì§œ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸° (v1.4)
    def start_fetch_press_releases_today(self):
        self.set_step1_buttons_state(tk.DISABLED, "ì˜¤ëŠ˜ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        self.press_tree.delete(*self.press_tree.get_children())
        self.tree_data.clear() 
        self.status_var.set("ì˜¤ëŠ˜ ë‚ ì§œ ë³´ë„ìë£Œ ëª©ë¡ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤...")
        # filter_today=True ì „ë‹¬
        threading.Thread(target=self.run_fetch_press_releases_task, args=(True,), daemon=True).start()

    # âœ… [v5.0.1 ì‹ ê·œ] Step 1 ë²„íŠ¼ ìƒíƒœ ê´€ë¦¬ í—¬í¼
    def set_step1_buttons_state(self, state, main_text=None):
        self.fetch_press_button.config(state=state)
        self.fetch_today_button.config(state=state)
        self.export_button.config(state=state if state == tk.DISABLED else tk.NORMAL) # ì—‘ì…€ ë²„íŠ¼ì€ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í™œì„±í™”ë¨
        
        if state == tk.DISABLED:
            if "ì˜¤ëŠ˜" in (main_text or ""):
                self.fetch_today_button.config(text=main_text)
            else:
                self.fetch_press_button.config(text=main_text)
        else:
            # ë³µêµ¬
            self.fetch_press_button.config(text="ë³´ë„ë°°í¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°(1í˜ì´ì§€)")
            today_str = datetime.now().strftime('%Y-%m-%d')
            self.fetch_today_button.config(text=f"'{today_str}' ì¼ì ëª©ë¡ ê°€ì ¸ì˜¤ê¸°")


    def run_fetch_press_releases_task(self, filter_today=False):
        # âœ… [v5.0.2 ìˆ˜ì •] v1.4ì˜ í›„ì²˜ë¦¬ ë¡œì§ ì ìš© (ì‹œê°„ ë®ì–´ì“°ê¸°, í•„í„°ë§)
        try:
            results = scrape_press_releases(self.goegh_base_url) 
            
            # [ìš”ì²­ 1] ë“±ë¡ì‹œê°„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œê°„(HH:MM)ìœ¼ë¡œ í†µì¼
            current_time_str = datetime.now().strftime('%H:%M')
            for item in results:
                item['time'] = current_time_str

            # [ìš”ì²­ 4] ì˜¤ëŠ˜ ë‚ ì§œ í•„í„°ë§ (filter_today=True ì¼ ë•Œ)
            if filter_today:
                today_date_str = datetime.now().strftime('%Y.%m.%d')
                print(f"[í•„í„°] ì˜¤ëŠ˜ ë‚ ì§œ({today_date_str}) ê²Œì‹œê¸€ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.")
                results = [item for item in results if item.get('date') == today_date_str]
            
            # âœ… [v5.0.2 ìˆ˜ì •] 
            # (v1.3) GUI í‘œì‹œëŠ” ì§€ì—­ë³„ ì •ë ¬ <-- ì´ ë¡œì§ì„ ì‚­ì œ!!
            # def sort_key_region(item):
            # ... (ì‚­ì œ) ...
            # results.sort(key=sort_key_region)
            #
            # -> GUIëŠ” í¬ë¡¤ë§í•œ ì›ë³¸ ìˆœì„œ(ìµœì‹ ìˆœ) ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
            
            self.root.after(0, self.update_press_release_treeview, results, filter_today)
            
        except Exception as e:
            print(f"â€¼ï¸ ë³´ë„ìë£Œ ìŠ¤í¬ë˜í•‘ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            self.root.after(0, self.update_press_release_treeview, [], filter_today)

    def update_press_release_treeview(self, results, filter_today=False):
        # (ì´ í•¨ìˆ˜ëŠ” v5.0.1ê³¼ ë™ì¼ - ìˆ˜ì • ì—†ìŒ)
        # âœ… [v5.0.1 ìˆ˜ì •] 
        # GUIëŠ” (Title, Date, Keyword) ìœ ì§€, ì—‘ì…€ìš© tree_dataëŠ” ì›ë³¸(results) ì €ì¥
        self.press_tree.delete(*self.press_tree.get_children())
        self.tree_data.clear()
        
        if not results:
            status_msg = "ì˜¤ëŠ˜ ë‚ ì§œ ë³´ë„ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤." if filter_today else "ë³´ë„ìë£Œ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆê±°ë‚˜, ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            self.status_var.set(status_msg)
        else:
            for item in results:
                # [v5.0.1] GUIì—ëŠ” v5.0 í˜•ì‹ (ì œëª©, ë‚ ì§œ, ì¶”ì²œí‚¤ì›Œë“œ) ì‚¬ìš©
                values_gui = (item["title"], item["date"], item["keyword_gui"])
                
                item_id = self.press_tree.insert("", tk.END, values=values_gui)
                # [v5.0.1] ì—‘ì…€ìš© ë°ì´í„°ëŠ” ì›ë³¸ itemì„ ì €ì¥ (v1.4)
                self.tree_data[item_id] = item 
            
            # âœ… [v5.0.2 ìˆ˜ì •] ìƒíƒœë°” ë©”ì‹œì§€ ë³€ê²½ (ì •ë ¬ ë¬¸êµ¬ ì‚­ì œ)
            status_msg = f"ì˜¤ëŠ˜ ë‚ ì§œ ë³´ë„ìë£Œ {len(results)}ê°œ ë¡œë“œ ì™„ë£Œ." if filter_today else f"ë³´ë„ìë£Œ {len(results)}ê°œ ë¡œë“œ ì™„ë£Œ. (ìµœì‹ ìˆœ)"
            self.status_var.set(status_msg)
            self.export_button.config(state=tk.NORMAL) # ì—‘ì…€ ë²„íŠ¼ í™œì„±í™”

        # [v5.0.1] Step 1 ë²„íŠ¼ ìƒíƒœ ë³µêµ¬
        self.set_step1_buttons_state(tk.NORMAL)
        
        # [v5.0.1] Step 2 ë²„íŠ¼ì€ í•­ìƒ í™œì„±í™” (Step 1ê³¼ ë…ë¦½ì ì´ë¯€ë¡œ)
        self.start_button.config(state=tk.NORMAL)

    # --- âœ… [v5.0.1 ì‹ ê·œ] Step 1 ì—‘ì…€ ì €ì¥ (v1.4 ë·°ì–´ ê¸°ì¤€) ---
    def export_to_excel(self):
        if not self.tree_data:
            messagebox.showwarning("ë°ì´í„° ì—†ìŒ", "ë¨¼ì € 'ëª©ë¡ ê°€ì ¸ì˜¤ê¸°'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return

        today_str = datetime.today().strftime('%Y%m%d_%H%M')
        filename = filedialog.asksaveasfilename(
            initialfile=f"ê´‘ì£¼í•˜ë‚¨_ë³´ë„ìë£Œ_{today_str}.xlsx",
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")]
        )

        if not filename:
            self.status_var.set("ì—‘ì…€ ì €ì¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return

        self.status_var.set(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì¤‘... ({os.path.basename(filename)})")
        # ëª¨ë“  ë²„íŠ¼ ë¹„í™œì„±í™” (ì €ì¥ ì¤‘)
        self.set_step1_buttons_state(tk.DISABLED, "ì—‘ì…€ ì €ì¥ ì¤‘...")
        self.start_button.config(state=tk.DISABLED)

        try:
            # tree_dataëŠ” item_id: item ë”•ì…”ë„ˆë¦¬. ê°’ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            # (v5.0.2) data_listëŠ” GUIì™€ ë™ì¼í•œ 'í¬ë¡¤ë§ ìˆœì„œ(ìµœì‹ ìˆœ)'ë¥¼ ìœ ì§€
            data_list = list(self.tree_data.values())
            
            # âœ… [v5.0.2 ìˆ˜ì •]
            # (v1.3) ì—‘ì…€ì€ ë‚ ì§œìˆœ ì •ë ¬ (GUI ì •ë ¬ê³¼ ë‹¤ë¦„) <-- ì´ ë¡œì§ì„ ì‚­ì œ
            # data_list.sort(key=lambda item: (item.get('date', ''), item.get('region', '')))
            
            # âœ… [v5.0.2 ì‹ ê·œ] ìš”ì²­ì‚¬í•­: ì—‘ì…€ì€ (êµìœ¡ì§€ì›ì²­ > ê´‘ì£¼ > í•˜ë‚¨) ìˆœì„œë¡œ ì •ë ¬.
            # (Pythonì˜ sort()ëŠ” stableí•˜ë¯€ë¡œ, ê¸°ì¡´ì˜ 'í¬ë¡¤ë§ ìˆœì„œ(ìµœì‹ ìˆœ)'ê°€ 2ì°¨ ì •ë ¬ë¡œ ìœ ì§€ë¨)
            def sort_key_excel_region(item):
                region = item.get('region', 'êµìœ¡ì§€ì›ì²­') # ê¸°ë³¸ê°’ì„ 'êµìœ¡ì§€ì›ì²­'ìœ¼ë¡œ
                if region == 'êµìœ¡ì§€ì›ì²­':
                    return 0
                if region == 'ê´‘ì£¼':
                    return 1
                if region == 'í•˜ë‚¨':
                    return 2
                return 3 # ê¸°íƒ€ (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°)

            data_list.sort(key=sort_key_excel_region)
            
            processed_list = []
            for i, item in enumerate(data_list):
                recommend_val = "â˜…" if i == 0 else item.get("priority", "")
                
                processed_list.append({
                    "recommend": recommend_val,
                    "yeonbeon": i + 1,
                    "region": item.get("region", ""),
                    "date": item.get("date", ""),
                    "time": item.get("time", ""), # ìŠ¤í¬ë˜í•‘ ì‹œì ì˜ ì‹œê°„
                    "title": item.get("title", ""),
                    "institution": item.get("institution", ""),
                    "notes": item.get("notes", "")
                })
            
            df = pd.DataFrame(processed_list)

            column_keys = ["recommend", "yeonbeon", "region", "date", "time", "title", "institution", "notes"]
            header_names = ["ì¶”ì²œ", "ì—°ë²ˆ", "ì§€ì—­", "ë“±ë¡ì¼", "ë“±ë¡ì‹œê°„", "ë³´ë„ì œëª©", "ë“±ë¡ê¸°ê´€", "ë¹„ê³ "]
            df = df[column_keys]

            with pd.ExcelWriter(filename, engine="xlsxwriter") as writer:
                workbook = writer.book
                worksheet = workbook.add_worksheet("ë³´ë„ìë£Œ") 

                # (v1.3) ì„œì‹ ì •ì˜
                header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D7E4BC', 'border': 1})
                title_format = workbook.add_format({'border': 1, 'align': 'left', 'valign': 'vcenter', 'text_wrap': True})
                center_format = workbook.add_format({'border': 1, 'align': 'center', 'valign': 'vcenter'})
                left_format = workbook.add_format({'border': 1, 'align': 'left', 'valign': 'vcenter'})

                for col_num, header in enumerate(header_names):
                    worksheet.write(0, col_num, header, header_format)
                
                worksheet.set_default_row(30) 
                worksheet.set_row(0, 20) 

                for row_num, row in enumerate(df.itertuples(index=False), start=1):
                    worksheet.write(row_num, 0, row.recommend, center_format) # A: ì¶”ì²œ
                    worksheet.write(row_num, 1, row.yeonbeon, center_format)  # B: ì—°ë²ˆ
                    worksheet.write(row_num, 2, row.region, center_format)    # C: ì§€ì—­
                    worksheet.write(row_num, 3, row.date, center_format)      # D: ë“±ë¡ì¼
                    worksheet.write(row_num, 4, row.time, center_format)      # E: ë“±ë¡ì‹œê°„
                    worksheet.write(row_num, 5, row.title, title_format)      # F: ë³´ë„ì œëª©
                    worksheet.write(row_num, 6, row.institution, left_format) # G: ë“±ë¡ê¸°ê´€
                    worksheet.write(row_num, 7, row.notes, left_format)       # H: ë¹„ê³ 

                # --- 5-5. [v1.4] ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì • ---
                worksheet.set_column("A:A", 5)  # ì¶”ì²œ
                worksheet.set_column("B:B", 5)  # ì—°ë²ˆ
                worksheet.set_column("C:C", 6)  # ì§€ì—­
                worksheet.set_column("D:D", 12) # ë“±ë¡ì¼
                worksheet.set_column("E:E", 8) # ë“±ë¡ì‹œê°„
                worksheet.set_column("F:F", 70) # ë³´ë„ì œëª©
                worksheet.set_column("G:G", 18) # ë“±ë¡ê¸°ê´€
                worksheet.set_column("H:H", 5) # ë¹„ê³ 
                
                worksheet.freeze_panes(1, 0) 
                worksheet.autofilter(0, 0, len(df), len(header_names) - 1)

            print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {filename}")
            
            self.root.after(0, lambda: [
                self.show_completion_window(filename, len(df)),
                self.status_var.set(f"ì €ì¥ ì™„ë£Œ: {os.path.basename(filename)}. ìƒˆ ì‘ì—… ê°€ëŠ¥."),
                # (v1.4) [ìˆ˜ì •] ì—‘ì…€ ì €ì¥ ì™„ë£Œ í›„ ëª¨ë“  ë²„íŠ¼ í™œì„±í™”
                self.set_step1_buttons_state(tk.NORMAL),
                self.start_button.config(state=tk.NORMAL)
            ])

        except Exception as e:
            print(f"\nâŒ ì—‘ì…€ ì €ì¥ ì˜¤ë¥˜ ë°œìƒ:")
            traceback.print_exc()
            self.root.after(0, lambda e=e: [ 
                messagebox.showerror("ì—‘ì…€ ì €ì¥ ì˜¤ë¥˜", f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}\n\n(íŒŒì¼ì´ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)"),
                self.status_var.set(f"ì—‘ì…€ ì €ì¥ ì˜¤ë¥˜: {e}."),
                # (v1.4) [ìˆ˜ì •] ì—‘ì…€ ì €ì¥ ì˜¤ë¥˜ ì‹œì—ë„ ëª¨ë“  ë²„íŠ¼ í™œì„±í™”
                self.set_step1_buttons_state(tk.NORMAL),
                self.start_button.config(state=tk.NORMAL)
            ])

    # --- [Step 2] 6ëŒ€ ì§€ë°©ì§€ ìŠ¤í¬ë ˆì´í¼ ìŠ¤ë ˆë“œ ê´€ë¦¬ (âœ… v5.0 ì›ë³¸ ìœ ì§€) ---
    
    def start_main_scraper_thread(self):
        self.status_var.set("ì…ë ¥ ê°’ ê²€ì¦ ì¤‘...")
        
        try:
            days_limit = int(self.days_entry.get())
            if days_limit <= 0: raise ValueError
        except ValueError:
            self.status_var.set("ì…ë ¥ ì˜¤ë¥˜: 'ìµœê·¼ Nì¼'ì€ 1 ì´ìƒì˜ ìˆ«ìë¡œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            messagebox.showerror("ì…ë ¥ ì˜¤ë¥˜", "ìµœê·¼ Nì¼ì€ 1 ì´ìƒì˜ ìˆ«ìë¡œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            return

        keywords = [kw.strip() for kw in self.keyword_text_var.get().split(",") if kw.strip()]
        if not keywords:
            self.status_var.set("ì…ë ¥ ì˜¤ë¥˜: 'ê²€ìƒ‰ì–´'ë¥¼ 1ê°œ ì´ìƒ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            messagebox.showerror("ì…ë ¥ ì˜¤ë¥˜", "ê²€ìƒ‰ì–´ë¥¼ 1ê°œ ì´ìƒ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            return
        
        if len(keywords) > 12:
            messagebox.showwarning("ì…ë ¥ í™•ì¸", "ê²€ìƒ‰ì–´ê°€ 12ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. (ìµœëŒ€ 12ê°œ)")
            keywords = keywords[:12]
            self.keyword_text_var.set(", ".join(keywords))
            
        # âœ… [v5.0] ì²´í¬ë°•ìŠ¤ ê°’ ì½ê¸°
        fetch_main6 = self.fetch_main6_var.get()
        fetch_other16 = self.fetch_other16_var.get()
        fetch_google = self.fetch_google_var.get()
        
        if not fetch_main6 and not fetch_other16 and not fetch_google:
            self.status_var.set("ì…ë ¥ ì˜¤ë¥˜: 'ê²€ìƒ‰ ëŒ€ìƒ'ì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
            messagebox.showerror("ì…ë ¥ ì˜¤ë¥˜", "ê²€ìƒ‰ ëŒ€ìƒì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
            return
        
        self.status_var.set(f"[{keywords[0]} ì™¸ {len(keywords)-1}ê°œ] (ìµœê·¼ {days_limit}ì¼) í†µí•© ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # âœ… [v5.0.1 ìˆ˜ì •] Step 2 ì‹œì‘ ì‹œ ëª¨ë“  ë²„íŠ¼ ë¹„í™œì„±í™”
        self.start_button.config(state=tk.DISABLED)
        self.word_button.config(state=tk.DISABLED)
        self.latest_articles_df = None
        self.set_step1_buttons_state(tk.DISABLED, "Step 2 ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        
        threading.Thread(target=self.run_main_scraper_task, 
                         args=(keywords, days_limit, fetch_main6, fetch_other16, fetch_google), 
                         daemon=True).start()

    def run_main_scraper_task(self, keywords, days_limit, fetch_main6, fetch_other16, fetch_google):
        # (v5.0 ì›ë³¸ê³¼ ë™ì¼)
        filename = ""
        all_articles = []
        
        # (name, function, type)
        functions_map_main6 = [
            ("ê¸°í˜¸ì¼ë³´", fetch_kiho_multi, "group"),
            ("ê²½ê¸°ì¼ë³´", fetch_kyeonggi_multi, "group"),
            ("ê²½ì¸ì¼ë³´", fetch_kyeongin_multi, "group"),
            ("ê²½ê¸°ì‹ ë¬¸", fetch_kgnews_multi, "group"),
            ("ì¸ì²œì¼ë³´", fetch_incheonilbo_multi, "group"),
            ("ì¤‘ë¶€ì¼ë³´", fetch_joongbu_multi, "group"),
        ]
        
        functions_map_other16 = [
            ("ê²½ê¸°í•«íƒ€ì„ìŠ¤", fetch_kghottimes_multi, "group"),
            ("í•˜ë‚¨íƒ€ì„ì¦ˆ", fetch_hanamtimes_multi, "group"),
            ("ì‹œí‹°ë‰´ìŠ¤", fetch_ctnews_multi, "group"),
            ("êµ¿íƒ€ì„ì¦ˆ", fetch_goodtimes_multi, "group"),
            ("í•˜ë‚˜ë¡œì‹ ë¬¸", fetch_hnrsm_multi, "group"),
            ("í•˜ë‚¨ì‹ ë¬¸", fetch_ehanam_multi, "group"),
            ("ê´‘ì£¼ì‹ ë¬¸", fetch_gjilbo_multi, "group"),
            ("ì¤‘ë¶€ì‹œì‚¬ì‹ ë¬¸", fetch_jungbusisa_multi, "group"),
            ("í¬íƒˆë‰´ìŠ¤í†µì‹ ", fetch_portalnews_multi, "group"),
            ("ê²½ê¸°ì—°í•©ë‰´ìŠ¤", fetch_kgyonhap_multi, "group"),
            ("êµì°¨ë¡œì €ë„", fetch_kocus_multi, "group"),
            ("ë¹„ì „21ë‰´ìŠ¤", fetch_vision21_multi, "group"),
            ("ë‰´ìŠ¤íˆ¬ë°ì´24", fetch_newstoday24_multi, "group"),
            ("íˆ¬ë°ì´ê´‘ì£¼í•˜ë‚¨", fetch_tgh_multi, "group"),
            ("ë¯¸ë””ì–´ë¦¬í¬íŠ¸", fetch_mediareport_multi, "group"),
            ("ìˆ˜ë„ê¶Œì¼ë³´", fetch_sudokwon_multi, "group"),
        ]
        
        functions_map_google = [
            ("Googleë‰´ìŠ¤(RSS)", fetch_google_news_feed, "individual"),
        ]
        
        try:
            self.root.after(0, lambda: self.status_var.set("ì‘ì—… ìŠ¤ë ˆë“œ ì‹œì‘... ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • ì¤‘..."))
            
            today = datetime.today()
            date_limit_dt = today - timedelta(days=days_limit)
            date_limit = date_limit_dt.replace(hour=0, minute=0, second=0, microsecond=0)
            
            print(f"--- ê²€ìƒ‰ ì‹œì‘ (í‚¤ì›Œë“œ {len(keywords)}ê°œ, ê²€ìƒ‰ ê¸°ê°„: {date_limit.strftime('%Y-%m-%d')} ~ {today.strftime('%Y-%m-%d')}) ---")
            
            # âœ… [v5.0] í†µí•© ì—‘ì…€ íŒŒì¼ëª…
            filename = f"í†µí•©_ë‰´ìŠ¤ê²€ìƒ‰_{today.strftime('%Y%m%d_%H%M')}.xlsx"
            
            self.root.after(0, lambda: self.status_var.set(f"ì—‘ì…€ íŒŒì¼ëª… ìƒì„±: {filename}"))

            futures_to_run = {}
            
            max_workers_selenium = 8
            max_workers_google = 4
            
            with ThreadPoolExecutor(max_workers=max_workers_selenium + max_workers_google) as executor:
                
                # --- 1. ì£¼ìš” 6ëŒ€ ì§€ë°©ì§€ ì‘ì—… ì œì¶œ (group) ---
                if fetch_main6:
                    print("--- [v5.0] ì£¼ìš” 6ëŒ€ ì§€ë°©ì§€ ê²€ìƒ‰ ì‹œì‘ ---")
                    for name, func, f_type in functions_map_main6:
                        future = executor.submit(func, keywords, date_limit, days_limit) 
                        futures_to_run[future] = (name, "ì£¼ìš”6ëŒ€")

                # --- 2. ê¸°íƒ€ 16ëŒ€ ì§€ë°©ì§€ ì‘ì—… ì œì¶œ (group) ---
                if fetch_other16:
                    print("--- [v5.0] ê¸°íƒ€ 16ê°œ ì§€ë°©ì§€ ê²€ìƒ‰ ì‹œì‘ ---")
                    for name, func, f_type in functions_map_other16:
                        future = executor.submit(func, keywords, date_limit, days_limit) 
                        futures_to_run[future] = (name, "ê¸°íƒ€16ëŒ€")
                
                # --- 3. Google ë‰´ìŠ¤ ì‘ì—… ì œì¶œ (individual) ---
                if fetch_google:
                    print("--- [v5.0] Google ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘ ---")
                    for name, func, f_type in functions_map_google:
                        # âœ… [v5.0.3 ìˆ˜ì •] individual íƒ€ì…ì€ funcì´ í‚¤ì›Œë“œ ëª©ë¡ì„ ì§ì ‘ ì²˜ë¦¬í•˜ë„ë¡ í•œë²ˆë§Œ ì œì¶œ
                        if f_type == "group" or f_type == "individual":
                            future = executor.submit(func, keywords, date_limit, days_limit)
                            futures_to_run[future] = (name, "Google")
                
                # --- 4. í†µí•© ê²°ê³¼ ì²˜ë¦¬ ---
                total_futures = len(futures_to_run)
                completed_count = 0
                
                print(f"--- [v5.0] ì´ {total_futures}ê°œì˜ ì‘ì—…ì„ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. ---")
                
                for future in as_completed(futures_to_run):
                    completed_count += 1
                    source_name, source_group = futures_to_run[future] 
                    
                    status_prefix = f"[ ({completed_count}/{total_futures}) {source_group} ]"
                    
                    try:
                        items = future.result() # itemsëŠ” list
                        
                        if not items:
                            print(f"ğŸ“ª [{source_name}] ì‘ì—… ë°˜í™˜ê°’ì´ ë¹„ì–´ìˆìŒ.")
                            self.root.after(0, lambda p=status_prefix, s=source_name: 
                                            self.status_var.set(f"{p} ğŸ“ª [{s}] ê²°ê³¼ ì—†ìŒ."))
                            continue

                        article_count = sum(1 for item in items if "ì—†ìŒ" not in item.get("ë³´ë„ì¼", "") and "ì˜¤ë¥˜" not in item.get("ë³´ë„ì¼", ""))
                        
                        if article_count == 0:
                            self.root.after(0, lambda p=status_prefix, s=source_name: 
                                            self.status_var.set(f"{p} ğŸ“ª [{s}] ê¸°ì‚¬ ì—†ìŒ."))
                        else:
                            self.root.after(0, lambda p=status_prefix, s=source_name, c=article_count: 
                                            self.status_var.set(f"{p} ğŸ“° [{s}] {c}ê±´ ë°œê²¬!"))
                        
                        all_articles.extend(items)

                    except Exception as e:
                        print(f"--- â€¼ï¸ {source_name} ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ ---")
                        traceback.print_exc() 
                        self.root.after(0, lambda p=status_prefix, s=source_name: 
                                        self.status_var.set(f"{p} âŒ [{s}] ì˜¤ë¥˜ ë°œìƒ."))
                
            # (as_completed ë£¨í”„ê°€ ëë‚œ í›„)
            
            if not all_articles:
                print("\nëª¨ë“  ê²€ìƒ‰ì–´ì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.root.after(0, lambda: [
                    messagebox.showinfo("ê²€ìƒ‰ ì™„ë£Œ", "ëª¨ë“  ê²€ìƒ‰ì–´ì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."),
                    self.status_var.set("ê²€ìƒ‰ ì™„ë£Œ. ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ìƒˆ ê²€ìƒ‰ ê°€ëŠ¥)"),
                    # âœ… [v5.0.1] ëª¨ë“  ë²„íŠ¼ í™œì„±í™”
                    self.start_button.config(state=tk.NORMAL),
                    self.set_step1_buttons_state(tk.NORMAL),
                    self.word_button.config(state=tk.DISABLED)
                ])
                return

            self.root.after(0, lambda: self.status_var.set(f"ì´ {len(all_articles)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ. ë°ì´í„° ì •ë ¬ ì¤‘..."))
            
            df = pd.DataFrame(all_articles)
            
            # 'ì—†ìŒ'/'ì˜¤ë¥˜' í•­ëª©ì€ ì •ë ¬ ì‹œ í•˜ë‹¨ìœ¼ë¡œ ë³´ë‚´ê¸° ìœ„í•´ ì„ì‹œ ë‚ ì§œ(1900ë…„) ë¶€ì—¬
            today_str = datetime.today().strftime('%Y-%m-%d')
            
            def clean_date_for_sorting(date_str):
                if isinstance(date_str, str):
                    if "ì—†ìŒ" in date_str or "ì˜¤ë¥˜" in date_str or "í™•ì¸ ë°”ëŒ" in date_str:
                        return pd.Timestamp('1900-01-01')
                    if re.match(r"^\d{4}-\d{2}-\d{2}$", date_str):
                        return pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')
                return pd.NaT # ê·¸ ì™¸ íŒŒì‹± ë¶ˆê°€ ë‚ ì§œ

            df['ë³´ë„ì¼_dt'] = df['ë³´ë„ì¼'].apply(clean_date_for_sorting)
            
            # âœ… [v5.0] ìš”ì²­ ì‚¬í•­: 'ë³´ë„ì œëª©' (ì˜¤ë¦„ì°¨ìˆœ)ìœ¼ë¡œ ì •ë ¬. 
            df = df.sort_values(by=['ë³´ë„ì œëª©', 'ë³´ë„ì¼_dt', 'ë³´ë„ë§¤ì²´'], ascending=[True, False, True])
            
            df = df.drop(columns=['ë³´ë„ì¼_dt']) 
            
            # [v5.0] ì—‘ì…€ ì—´ ìˆœì„œ (ê³µí†µ)
            df = df[["ë³´ë„ì¼", "ë³´ë„ë§¤ì²´", "ë³´ë„ì œëª©", "ë§í¬", "ê²€ìƒ‰ì–´"]]
            self.latest_articles_df = df.copy()

            print("\nğŸ’¾ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì¤‘...")
            self.root.after(0, lambda: self.status_var.set(f"ë°ì´í„° ì •ë ¬ ì™„ë£Œ. ì—‘ì…€ íŒŒì¼({filename}) ì €ì¥ ì¤‘..."))
            
            with pd.ExcelWriter(filename, engine="xlsxwriter") as writer:
                df.to_excel(writer, sheet_name="í†µí•©ê²°ê³¼", index=False, startrow=1, header=False)
                workbook = writer.book
                worksheet = writer.sheets["í†µí•©ê²°ê³¼"]
                
                header_format = workbook.add_format({
                    'bold': True, 'text_wrap': True, 'valign': 'vcenter',
                    'fg_color': '#D7E4BC', 'border': 1 
                })
                
                headers = ["ë³´ë„ì¼", "ë³´ë„ë§¤ì²´", "ë³´ë„ì œëª©", "ë§í¬", "ê²€ìƒ‰ì–´"]
                for col_num, header in enumerate(headers):
                    worksheet.write(0, col_num, header, header_format)
                
                worksheet.freeze_panes(1, 0)
                url_format = workbook.add_format({'font_color': 'blue', 'underline': 1})
                
                for row_num, row in enumerate(df.itertuples(index=False), start=1):
                    worksheet.write(row_num, 0, row.ë³´ë„ì¼)
                    worksheet.write(row_num, 1, row.ë³´ë„ë§¤ì²´)
                    
                    if (row.ë§í¬ and isinstance(row.ë§í¬, str) and 
                        row.ë§í¬.startswith("http") and 
                        "ì—†ìŒ" not in row.ë³´ë„ì¼ and 
                        "ì˜¤ë¥˜" not in row.ë³´ë„ì¼ and
                        "í™•ì¸ ë°”ëŒ" not in row.ë³´ë„ì¼):
                        try:
                            worksheet.write_url(row_num, 2, row.ë§í¬, string=row.ë³´ë„ì œëª©, cell_format=url_format)
                        except ValueError as excel_url_error:
                            print(f"ì—‘ì…€ URL ì“°ê¸° ì˜¤ë¥˜ (í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´): {excel_url_error} | ë§í¬: {row.ë§í¬}")
                            worksheet.write(row_num, 2, row.ë³´ë„ì œëª©) # ì˜¤ë¥˜ ì‹œ ì œëª©ë§Œ í…ìŠ¤íŠ¸ë¡œ
                    else:
                        worksheet.write(row_num, 2, row.ë³´ë„ì œëª©) 
                        
                    worksheet.write(row_num, 3, row.ë§í¬) 
                    worksheet.write(row_num, 4, row.ê²€ìƒ‰ì–´)

                worksheet.set_column("A:A", 12) # ë³´ë„ì¼
                worksheet.set_column("B:B", 18) # ë³´ë„ë§¤ì²´
                worksheet.set_column("C:C", 70) # ë³´ë„ì œëª©
                worksheet.set_column("D:D", 40) # ë§í¬
                worksheet.set_column("E:E", 15) # ê²€ìƒ‰ì–´
                worksheet.autofilter(0, 0, len(df), len(headers) - 1)

            print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {filename}")
            
            # âœ… [v5.0.1] ì™„ë£Œ íŒì—… ë° ëª¨ë“  ë²„íŠ¼ í™œì„±í™”
            self.root.after(0, lambda: [
                self.show_completion_window(filename, len(all_articles)),
                self.status_var.set(f"ì €ì¥ ì™„ë£Œ: {filename}. (ì´ {len(all_articles)}ê°œ). ìƒˆ ê²€ìƒ‰ ê°€ëŠ¥."),
                self.start_button.config(state=tk.NORMAL),
                self.set_step1_buttons_state(tk.NORMAL),
                self.word_button.config(state=tk.NORMAL)
            ])

        except Exception as e:
            print("\nâŒ ì˜ˆì™¸ ë°œìƒ:")
            traceback.print_exc()
            self.root.after(0, lambda e=e: [
                messagebox.showerror("ì¹˜ëª…ì  ì˜¤ë¥˜", f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}\n\n(ìì„¸í•œ ë‚´ìš©ì€ ì½˜ì†” ì°½ì„ í™•ì¸í•˜ì„¸ìš”)"),
                self.status_var.set(f"ì˜¤ë¥˜ ë°œìƒ: {e}. (ì½˜ì†” ì°½ í™•ì¸)"),
                # âœ… [v5.0.1] ëª¨ë“  ë²„íŠ¼ í™œì„±í™”
                self.start_button.config(state=tk.NORMAL),
                self.set_step1_buttons_state(tk.NORMAL),
                self.word_button.config(state=tk.DISABLED)
            ])

    # --- Word ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ ---

    def start_word_export_thread(self):
        if not isinstance(self.latest_articles_df, pd.DataFrame) or self.latest_articles_df.empty:
            messagebox.showinfo("Word ë¬¸ì„œ ìƒì„±", "ë¨¼ì € 'í†µí•© ê²€ìƒ‰ ë° ì—‘ì…€ ì €ì¥'ì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ì£¼ì„¸ìš”.")
            return

        df_snapshot = self.latest_articles_df.copy()
        self.word_button.config(state=tk.DISABLED)
        self.status_var.set("Word ë¬¸ì„œë¥¼ ìƒì„± ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")

        threading.Thread(target=self.run_word_export_task, args=(df_snapshot,), daemon=True).start()

    def run_word_export_task(self, df_snapshot):
        driver = None
        try:
            self.root.after(0, lambda: self.status_var.set("Word ë¬¸ì„œ ìƒì„± ì¤‘... (ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì´ˆê¸°í™”)"))
            driver = setup_driver_compatible(headless=True)
            if not driver:
                raise Exception("ì›¹ ë“œë¼ì´ë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.root.after(0, lambda: self.status_var.set("Word ë¬¸ì„œ ìƒì„± ì¤‘... (ë§í¬ ë³€í™˜ ë° ë¬¸ì„œ ìƒì„±)"))
            info = self._create_word_summary_document(df_snapshot, driver)
            
            self.root.after(0, lambda info=info: [
                self.show_completion_window(info["filename"], info["article_count"]),
                self.status_var.set(f"Word ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {os.path.basename(info['filename'])}"),
                self.word_button.config(state=tk.NORMAL)
            ])
        except Exception as e:
            print("\nâŒ Word ë¬¸ì„œ ìƒì„± ì˜¤ë¥˜:")
            traceback.print_exc()
            self.root.after(0, lambda e=e: [
                messagebox.showerror("Word ì €ì¥ ì˜¤ë¥˜", f"Word ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}"),
                self.status_var.set(f"Word ì €ì¥ ì˜¤ë¥˜: {e}"),
                self.word_button.config(state=tk.NORMAL)
            ])
        finally:
            if driver:
                print("Word ë¬¸ì„œ ìƒì„±ì„ ìœ„í•œ ì„ì‹œ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                driver.quit()

    def _create_word_summary_document(self, df, driver):
        latest_dt, latest_label = self._get_latest_date_info(df)
        doc_filename = f"ì¼ì¼ë³´ë„_ìš”ì•½_{latest_dt.strftime('%Y%m%d_%H%M')}.docx"
        output_path = os.path.abspath(doc_filename)

        document = Document()
        self._configure_word_styles(document)

        title_paragraph = document.add_heading(f"ğŸ“° ê´‘ì£¼í•˜ë‚¨êµìœ¡ ë‰´ìŠ¤ ë³´ë„ê¸°ì‚¬ ({latest_label} ì)", level=1)
        self._set_paragraph_font(title_paragraph, 18)

        for _ in range(2):
            spacer = document.add_paragraph("")
            self._set_paragraph_font(spacer, 13)

        grouped_entries = self._group_articles_for_word(df)

        for entry in grouped_entries:
            line_paragraph = document.add_paragraph(f"- {entry['title']}  {entry['media_text']}")
            self._set_paragraph_font(line_paragraph, 13)
            line_paragraph.paragraph_format.space_after = Pt(4)

            if entry["link"]:
                clean_link = self._resolve_google_news_url(entry["link"], driver)
                link_paragraph = document.add_paragraph("   â†ª ")
                self._set_paragraph_font(link_paragraph, 11)
                self._add_hyperlink(link_paragraph, clean_link, clean_link)
                link_paragraph.paragraph_format.space_after = Pt(6)

            document.add_paragraph("")

        document.save(output_path)
        return {"filename": output_path, "article_count": len(df)}

    def _configure_word_styles(self, document):
        normal_style = document.styles["Normal"]
        normal_style.font.name = "Malgun Gothic"
        normal_style._element.rPr.rFonts.set(qn("w:eastAsia"), "Malgun Gothic")
        normal_style.font.size = Pt(12)

        heading_style = document.styles["Heading 1"]
        heading_style.font.name = "Malgun Gothic"
        heading_style._element.rPr.rFonts.set(qn("w:eastAsia"), "Malgun Gothic")

    def _set_paragraph_font(self, paragraph, size):
        runs = paragraph.runs
        if not runs:
            runs = [paragraph.add_run("")]
        for run in runs:
            run.font.name = "Malgun Gothic"
            try:
                run._element.rPr.rFonts.set(qn("w:eastAsia"), "Malgun Gothic")
            except AttributeError:
                pass
            run.font.size = Pt(size)

    def _group_articles_for_word(self, df):
        grouped = []
        for _, row in df.iterrows():
            title = str(row.get("ë³´ë„ì œëª©", "") or "").strip()
            if not title:
                continue
            media = str(row.get("ë³´ë„ë§¤ì²´", "") or "").strip()
            link = str(row.get("ë§í¬", "") or "").strip()

            matched_group = None
            for group in grouped:
                existing_title = group["titles"][0]
                if self._calculate_title_similarity(title, existing_title) >= 0.6:
                    matched_group = group
                    break

            if not matched_group:
                matched_group = {"titles": [title], "medias": [], "links": []}
                grouped.append(matched_group)
            elif title not in matched_group["titles"]:
                matched_group["titles"].append(title)

            matched_group["medias"].append(media)
            matched_group["links"].append(link)

        structured = []
        for group in grouped:
            if not group["titles"]:
                continue
            representative_media, media_text = self._determine_media_text(group["medias"])
            final_link = self._pick_representative_link(group["medias"], group["links"], representative_media)
            structured.append({
                "title": group["titles"][0],
                "media_text": media_text,
                "link": final_link
            })

        return structured

    def _determine_media_text(self, medias):
        unique_media = []
        for media in medias:
            media = media.strip() if isinstance(media, str) else ""
            if media and media not in unique_media:
                unique_media.append(media)

        non_hanaro = [m for m in unique_media if m != "í•˜ë‚˜ë¡œì‹ ë¬¸"]

        if not unique_media:
            return "", "<ë§¤ì²´ ì •ë³´ ì—†ìŒ>"

        if len(unique_media) == 1 and unique_media[0] == "í•˜ë‚˜ë¡œì‹ ë¬¸":
            return "í•˜ë‚˜ë¡œì‹ ë¬¸", "<í•˜ë‚˜ë¡œì‹ ë¬¸>"

        if not non_hanaro:
            return "í•˜ë‚˜ë¡œì‹ ë¬¸", "<í•˜ë‚˜ë¡œì‹ ë¬¸>"

        if len(non_hanaro) == 1:
            return non_hanaro[0], f"<{non_hanaro[0]}>"

        return non_hanaro[0], f"<{non_hanaro[0]} ì™¸ {len(non_hanaro) - 1}ê±´>"

    def _pick_representative_link(self, medias, links, representative_media):
        if representative_media:
            for media, link in zip(medias, links):
                if media == representative_media and link:
                    return link
        for link in links:
            if link:
                return link
        return ""

    def _get_latest_date_info(self, df):
        date_series = pd.to_datetime(df["ë³´ë„ì¼"], errors="coerce")
        latest_dt = date_series.dropna().max()
        if pd.isna(latest_dt):
            latest_dt = datetime.today()
        if isinstance(latest_dt, pd.Timestamp):
            latest_dt = latest_dt.to_pydatetime()
        label = f"{latest_dt.year}. {latest_dt.month}. {latest_dt.day}."
        return latest_dt, label

    def _resolve_google_news_url(self, url, driver):
        if not url or not driver:
            return url or ""
            
        url = url.strip()
        # Check if it's a google news url that needs resolving
        if "news.google.com/rss/articles/" not in url:
            return url

        try:
            print(f"Google ë‰´ìŠ¤ ë§í¬ ê°ì§€, Seleniumìœ¼ë¡œ ìµœì¢… ì£¼ì†Œ í™•ì¸: {url}")
            driver.get(url)
            # Wait up to 10 seconds for the URL to change from the original google one.
            # This handles JS redirects.
            WebDriverWait(driver, 10).until(
                lambda d: d.current_url != url and "news.google.com" not in d.current_url
            )
            final_url = driver.current_url
            print(f"ë§í¬ ë³€í™˜ ì„±ê³µ (Selenium): {url} -> {final_url}")
            return final_url
        except TimeoutException:
            # If the URL doesn't change, it might be a direct link or a failed redirect.
            # Return the URL we have, which might still be the google one.
            current_url = driver.current_url
            print(f"Selenium íƒ€ì„ì•„ì›ƒ: URLì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ URL ë°˜í™˜: {current_url}")
            return current_url
        except Exception as e:
            print(f"Seleniumìœ¼ë¡œ ë§í¬ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì›ë³¸ URL ë°˜í™˜): {e}")
            return url # Fallback to original URL on error

    def _calculate_title_similarity(self, s1, s2):
        if not s1 or not s2:
            return 0.0
        distance = self._levenshtein_distance(s1, s2)
        max_len = max(len(s1), len(s2))
        return 1.0 - distance / max_len if max_len else 1.0

    def _levenshtein_distance(self, a, b):
        if a == b:
            return 0
        if not a:
            return len(b)
        if not b:
            return len(a)

        previous_row = list(range(len(b) + 1))
        for i, ca in enumerate(a, start=1):
            current_row = [i]
            for j, cb in enumerate(b, start=1):
                insertions = previous_row[j] + 1
                deletions = current_row[j - 1] + 1
                substitutions = previous_row[j - 1] + (ca != cb)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]

    def _add_hyperlink(self, paragraph, url, text):
        if not url:
            paragraph.add_run(text)
            return

        part = paragraph.part
        r_id = part.relate_to(url, RELATIONSHIP_TYPE.HYPERLINK, is_external=True)

        hyperlink = OxmlElement("w:hyperlink")
        hyperlink.set(qn("r:id"), r_id)

        new_run = OxmlElement("w:r")
        rPr = OxmlElement("w:rPr")
        rStyle = OxmlElement("w:rStyle")
        rStyle.set(qn("w:val"), "Hyperlink")
        rPr.append(rStyle)
        new_run.append(rPr)

        text_elem = OxmlElement("w:t")
        text_elem.text = text
        new_run.append(text_elem)
        hyperlink.append(new_run)

        paragraph._p.append(hyperlink)

# =============================================================================
# 8. GUI ì‹œì‘ (v5.0.1) -> (v5.0.2ë¡œ ì´ë¦„ë§Œ ë³€ê²½)
# =============================================================================
if __name__ == "__main__":
    try:
        # âœ… [v5.0.2 ìˆ˜ì •] ë²„ì „ëª… ë³€ê²½
        print("í”„ë¡œê·¸ë¨ ì‹œì‘... (v5.0.2) GUIë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.") 
        print("(ì²« ì‹¤í–‰ ì‹œ WebDriverManagerê°€ í¬ë¡¬ ë“œë¼ì´ë²„ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")
        
        get_driver_path() 
        print("ì›¹ ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ. GUIë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        root = tk.Tk()
        app = NewsScraperApp(root)
        root.mainloop()
        
    except Exception as e:
        print("\nâŒ GUI ì‹œì‘ ì˜¤ë¥˜ ë°œìƒ:")
        traceback.print_exc()
        try:
            temp_root = tk.Tk()
            temp_root.withdraw()
            messagebox.showerror("GUI ì‹œì‘ ì˜¤ë¥˜", 
                               f"í”„ë¡œê·¸ë¨ ì‹œì‘ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
                               f"ì˜¤ë¥˜: {e}\n"
                               f"(ìì„¸í•œ ë‚´ìš©ì€ ì½˜ì†” ì°½ì„ í™•ì¸í•´ì£¼ì„¸ìš”.)")
            temp_root.destroy()
        except tk.TclError:
             print("--- GUI(Tkinter) ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨ ---")
             input("ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤...")
